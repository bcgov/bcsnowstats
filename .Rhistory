"normal_Q10" = NA,
"normal_Q25"  = NA,
"normal_Q50" = NA,
"normal_Q75" = NA,
"normal_Q90" = NA,
"normal_maximum" = NA,
"data_range_normal" = NA,
"initial_normal_range" = paste0(normal_min, " to ", normal_max),
"normal_datarange_estimated" = NA,
"normal_datarange_raw" = NA,
"date_min_normal_utc" = NA,
"date_max_normal_utc"  = NA)
}
df_normals_out
}
df_normals_out  <- int_aswenorm(data = df, normal_max, normal_min, data_id)
library(bcsnowstats)
library(bcsnowstats)
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
library(dplyr)
sites <- c("3B17P", "3B23P", "3B24P", "3B25P", "3B26P")
normals_vancouverisland <- SWE_normals(data = sites,
normal_max = 2020,
normal_min = 1991,
force = TRUE)
View(normals_vancouverisland)
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
library(dplyr)
# Test for interactive plots
# V drive for saving
drive = "\\\\DRAIN.dmz\\Shared"
drive_G = "\\\\Backhoe\\s63101\\Watershare\\rfc"
drive_Q = "\\\\question.bcgov\\envwwwt\\rfc"
drive_R = "\\\\answer.bcgov\\envwww\\rfc"
sites <- c("3B17P", "3B23P", "3B24P", "3B25P", "3B26P")
normals_vancouverisland <- SWE_normals(data = sites,
normal_max = 2020,
normal_min = 1991,
force = TRUE)
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
library(dplyr)
drive = "\\\\DRAIN.dmz\\Shared"
drive_G = "\\\\Backhoe\\s63101\\Watershare\\rfc"
drive_Q = "\\\\question.bcgov\\envwwwt\\rfc"
drive_R = "\\\\answer.bcgov\\envwww\\rfc"
ASWE_sites <- bcsnowdata::snow_auto_location() %>%
dplyr::filter(STATUS == "Active")
ASWE_sites_active <- ASWE_sites$LOCATION_ID
ASWE_sites_active
normals_all <- SWE_normals(data = ASWE_sites_active,
normal_max = 2020,
normal_min = 1991,
force = TRUE)
sites <- c("3B17P", "3B23P", "3B24P", "3B25P", "3B26P")
normals_all <- SWE_normals(data = sites,
normal_max = 2020,
normal_min = 1991,
force = TRUE)
View(normals_all)
normals_all <- SWE_normals(data = ASWE_sites_active,
normal_max = 2020,
normal_min = 1991,
force = TRUE)
rlang::last_error(
)
normals_all <- SWE_normals(data = ASWE_sites_active[1:10],
normal_max = 2020,
normal_min = 1991,
force = TRUE)
normals_all <- SWE_normals(data = ASWE_sites_active[11:20],
normal_max = 2020,
normal_min = 1991,
force = TRUE)
View(normals_all)
normals_all <- SWE_normals(data = ASWE_sites_active[21:30],
normal_max = 2020,
normal_min = 1991,
force = TRUE)
normals_all <- SWE_normals(data = ASWE_sites_active[21:25],
normal_max = 2020,
normal_min = 1991,
force = TRUE)
View(normals_all)
normals_all <- SWE_normals(data = ASWE_sites_active[26:28],
normal_max = 2020,
normal_min = 1991,
force = TRUE)
normals_all <- SWE_normals(data = ASWE_sites_active[26],
normal_max = 2020,
normal_min = 1991,
force = TRUE)
data = ASWE_sites_active[26]
normal_max = 2020
normal_min = 1991
force = TRUE
aswe <- bcsnowdata::snow_auto_location()$LOCATION_ID
manual <- bcsnowdata::snow_manual_location()$LOCATION_ID
# if the user input data as a station name (i.e., the function is being used as a stand alone function), get the data for the station
if (all(data %in% aswe)) {
data_norm <- bcsnowdata::get_aswe_databc(
station_id = data,
get_year = "All",
parameter = "swe",
timestep = "daily") %>%
dplyr::rename("values_stats" = value)
id <- data
} else if (all(data %in% manual)) {
data_norm <- bcsnowdata::get_manual_swe(
station_id = data,
get_year = "All",
survey_period = "All")
id <- data
} else {
data_norm <- data
if ("value" %in% colnames(data_norm)) {
data_norm <- data_norm %>%
dplyr::rename("values_stats" = value)
}
id <- unique(data_norm$id)
}
any(id %in% aswe)
# filter data for ASWE sites
data_swe <- data_norm %>%
dplyr::filter(id %in% aswe)
df = data_swe
data_id = "values_stats"
data_dir <- function() {
if (R.Version()$major >= 4) {
getOption("bcsnowstats.data_dir", default = tools::R_user_dir("bcsnowstats", "cache"))
} else {
getOption("bcsnowstats.data_dir", default = rappdirs::user_cache_dir("bcsnowstats"))
}
}
show_cached_files <- function() {
file.path(list.files(data_dir(), full.names = TRUE))
}
check_write_to_data_dir <- function(dir, ask) {
if (ask) {
ans <- gtools::ask(paste("bcsnowstats would like to store this layer in the directory:",
dir, "Is that okay?", sep = "\n"))
if (!(ans %in% c("Yes", "YES", "yes", "y"))) stop("Exiting...", call. = FALSE)
}
if (!dir.exists(dir)) {
message("Creating directory to hold bcsnowstats data at ", dir)
dir.create(dir, showWarnings = FALSE, recursive = TRUE)
} else {
message("Saving to bcsnowstats data directory at ", dir)
}
}
# Check to ensure that the ASWE archived data has been cached on the user's computer and is up to date
fname <- paste0(unique(df$parameter), "_norm_archive.rds")
dir <- data_dir()
fpath <- file.path(dir, fname)
any(!file.exists(fpath)) | force
# Check that the directory exists
check_write_to_data_dir(dir, ask)
ask = FALSE
# Check that the directory exists
check_write_to_data_dir(dir, ask)
# Calculate the normal data for all days of the year
df_normals_out  <- int_aswenorm(data = df, normal_max, normal_min, data_id)
data = df
# Put data into right format using the data_massage function
data_m <- data_massage(data)
if ("swe_mean" %in% colnames(data_m)) {
data_id <- "swe_mean" # reassign the data_ID value
}
# Filter the data by the normal span that you specify
df_normal_time <- data_m %>%
dplyr::filter(wr <= normal_max, wr >= normal_min) %>% # Filter by the normal dates that you specify
dplyr::group_by(id, m_d) %>%
dplyr::rename(values_stats = all_of(data_id))
# ++++++++++++++++++++++ thresholds
# Check to see whether there is sufficient data to calculate a normal.
# The WMO recommends only calculating a normal for stations that have 80% of the data available
# Firstly, just show the amount of data available for the normal period
# Number of years with 80% or great of the data available.
# Only count the data between Oct-June - doesn't matter if the snow data is missing in summer - 273 days in snow accumulation/melt season
# Only for ASWE
df_normal_80 <- df_normal_time %>%
dplyr::filter(!is.na(values_stats)) %>% # # filter out missing data
dplyr::ungroup() %>%
dplyr::group_by(id, wr) %>%
dplyr::filter(lubridate::month(as.Date(m_d, format = "%m-%d")) <= 6 || lubridate::month(as.Date(m_d, format = "%m-%d")) >= 10) %>% # get only the snow accumulation and melt season
dplyr::mutate(percent_available = length(values_stats) / length(seq(as.Date("2020-10-01"), as.Date("2021-06-30"), by = "day")) * 100) %>%
dplyr::select(id, wr, percent_available) %>%
unique() %>%
dplyr::filter(percent_available >= 80) # filter by the 80% WMO threshold
# Get the number of years within the normal range with >= 80% data coverage within a specific year
ny_80 <- df_normal_80 %>%
dplyr::group_by(id) %>%
dplyr::summarize(numberofyears_80_raw = n())
# Add the number of years with 80% of data to the dataframe
df_nt <- df_normal_time %>%
dplyr::full_join(ny_80) %>%
dplyr::mutate(numberofyears_80_raw = ifelse(is.na(numberofyears_80_raw), 0, numberofyears_80_raw))
normals <- lapply(unique(data_m$id),
calc_norm,
df_nt,
df_normal_80, normal_max = normal_max, normal_min = normal_min)
# Function for defining whether to fill in data and calculate normals. To be run station by station
calc_norm <- function(station, df_nt, df_normal_80, normal_max, normal_min) {
numberofyears_80 <- df_nt %>%
ungroup() %>%
dplyr::filter(id %in% station) %>%
dplyr::select(numberofyears_80_raw) %>%
unique()
df_normal_time <- df_nt %>%
dplyr::filter(id %in% station)
dfn_80 <- df_normal_80 %>%
dplyr::filter(id %in% station)
if (dim(numberofyears_80)[1] == 0) {
numberofyears_80_raw <- 0
} else {
numberofyears_80_raw <- numberofyears_80$numberofyears_80_raw
}
# =============================
# Fill in data depending on how many years of data there are available
# Is there less than 10 years of data?
if (numberofyears_80_raw < 10) {
data_0t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# ++++++++++++++++++++++++++++++++++++++++++++++
# Use function to check to see if there is manual site to extend data.
# For now, do not calculate a normal
# Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columms
all_swe <- data_0t10 %>%
dplyr::filter(wr %in% dfn_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# Does the station have between 10-20 years of data? If so, extend the dataset using 1) manual dataset (if converted), and 2) adjacent stations
if (numberofyears_80_raw >= 10 && numberofyears_80_raw < 20) {
data_20t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# Fill in missing data with an estimated dataset from either manual dataset (if converted) and/or adjacent stations
all_swe <- snow_datafill(data_soi = data_20t10, data_id, normal_max, normal_min)
}
# Does the site have between 20-30 years of data? Don't add in any additional data and jsut calculcate normals from
if (numberofyears_80_raw >= 20 && numberofyears_80_raw <= 30) {
# DON'T Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columns
all_swe <- df_normal_time %>%
#dplyr::filter(wr %in% df_normal_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80$numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# End of data filling according to thresholds
# ==============================
# Calculate normals. Only calculate normal if there is sufficient data
if (length(all_swe$numberofyears_estimated_80) > 0 && unique(all_swe$numberofyears_estimated_80) >= 20 && unique(all_swe$numberofyears_estimated_80) <= 30) {
all_swe_1 <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::filter(!is.na(swe_fornormal))
# Calculate the normal statistics for each day of the year
df_normals <- do.call(data.frame,
list(dplyr::summarise(all_swe_1, normal_minimum = min(swe_fornormal, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_swe_mean = mean(swe_fornormal, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q5 = quantile(swe_fornormal, 0.05, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q10 = quantile(swe_fornormal, 0.1, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q25 = quantile(swe_fornormal, 0.25, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q50 = quantile(swe_fornormal, 0.5, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q75 = quantile(swe_fornormal, 0.75, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q90 = quantile(swe_fornormal, 0.90, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_maximum = max(swe_fornormal, na.rm = TRUE), .groups = "keep"))) %>%
dplyr::select(-m_d.1, -m_d.2, -m_d.3, -m_d.4, -m_d.5, -m_d.6, -m_d.7, -m_d.8) %>%
dplyr::select(-id.1, -id.2, -id.3, -id.4, -id.5, -id.6, -id.7, -id.8) %>%
#dplyr::mutate(Data_Range_normal = (paste0(round(normal_minimum, digits = 0), ' to ', round(normal_maximum, digits = 0)))) %>%
dplyr::mutate(data_range_normal = (paste0(min(lubridate::year(all_swe$date_utc), na.rm = TRUE), " to ", max(lubridate::year(all_swe$date_utc), na.rm = TRUE)))) %>%
dplyr::mutate(normal_datarange_estimated = unique(all_swe$numberofyears_estimated_80, na.rm = TRUE)[!is.na(unique(all_swe$numberofyears_estimated_80, na.rm = TRUE))]) %>%
dplyr::mutate(normal_datarange_raw = unique(all_swe$numberofyears_80_raw, na.rm = TRUE)[!is.na(unique(all_swe$numberofyears_80_raw, na.rm = TRUE))])
# get the day of the max and min!! Use only 'real', non estimated data
min_date <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.min(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_min_normal_utc = date_utc)
max_date <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.max(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_max_normal_utc = date_utc)
# append to data
dates <- dplyr::full_join(min_date, max_date, by = c("id", "m_d"))
df_normals_out <- dplyr::full_join(df_normals, dates, by = c("id", "m_d")) %>%
dplyr::mutate(initial_normal_range = paste0(normal_min, " to ", normal_max))
# Smooth all the statistics by the 5 -day average?
# If there is less than 10 years of data available even after trying adjacent sites, return
} else {
df_normals_out <- data.frame("id"  = station,
"m_d" = NA,
"normal_minimum" = NA,
"normal_swe_mean" = NA,
"normal_Q5" = NA,
"normal_Q10" = NA,
"normal_Q25"  = NA,
"normal_Q50" = NA,
"normal_Q75" = NA,
"normal_Q90" = NA,
"normal_maximum" = NA,
"data_range_normal" = NA,
"initial_normal_range" = paste0(normal_min, " to ", normal_max),
"normal_datarange_estimated" = NA,
"normal_datarange_raw" = NA,
"date_min_normal_utc" = NA,
"date_max_normal_utc"  = NA)
}
df_normals_out
}
# ++++++++++++++++++++++ thresholds
# Check to see whether there is sufficient data to calculate a normal.
# The WMO recommends only calculating a normal for stations that have 80% of the data available
# Firstly, just show the amount of data available for the normal period
# Number of years with 80% or great of the data available.
# Only count the data between Oct-June - doesn't matter if the snow data is missing in summer - 273 days in snow accumulation/melt season
# Only for ASWE
df_normal_80 <- df_normal_time %>%
dplyr::filter(!is.na(values_stats)) %>% # # filter out missing data
dplyr::ungroup() %>%
dplyr::group_by(id, wr) %>%
dplyr::filter(lubridate::month(as.Date(m_d, format = "%m-%d")) <= 6 || lubridate::month(as.Date(m_d, format = "%m-%d")) >= 10) %>% # get only the snow accumulation and melt season
dplyr::mutate(percent_available = length(values_stats) / length(seq(as.Date("2020-10-01"), as.Date("2021-06-30"), by = "day")) * 100) %>%
dplyr::select(id, wr, percent_available) %>%
unique() %>%
dplyr::filter(percent_available >= 80) # filter by the 80% WMO threshold
# Get the number of years within the normal range with >= 80% data coverage within a specific year
ny_80 <- df_normal_80 %>%
dplyr::group_by(id) %>%
dplyr::summarize(numberofyears_80_raw = n())
# Add the number of years with 80% of data to the dataframe
df_nt <- df_normal_time %>%
dplyr::full_join(ny_80) %>%
dplyr::mutate(numberofyears_80_raw = ifelse(is.na(numberofyears_80_raw), 0, numberofyears_80_raw))
normals <- lapply(unique(data_m$id),
calc_norm,
df_nt,
df_normal_80, normal_max = normal_max, normal_min = normal_min)
unique(data_m$id)
station= "1D18P"
station
numberofyears_80 <- df_nt %>%
ungroup() %>%
dplyr::filter(id %in% station) %>%
dplyr::select(numberofyears_80_raw) %>%
unique()
df_normal_time <- df_nt %>%
dplyr::filter(id %in% station)
dfn_80 <- df_normal_80 %>%
dplyr::filter(id %in% station)
if (dim(numberofyears_80)[1] == 0) {
numberofyears_80_raw <- 0
} else {
numberofyears_80_raw <- numberofyears_80$numberofyears_80_raw
}
# =============================
# Fill in data depending on how many years of data there are available
# Is there less than 10 years of data?
if (numberofyears_80_raw < 10) {
data_0t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# ++++++++++++++++++++++++++++++++++++++++++++++
# Use function to check to see if there is manual site to extend data.
# For now, do not calculate a normal
# Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columms
all_swe <- data_0t10 %>%
dplyr::filter(wr %in% dfn_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# Does the station have between 10-20 years of data? If so, extend the dataset using 1) manual dataset (if converted), and 2) adjacent stations
if (numberofyears_80_raw >= 10 && numberofyears_80_raw < 20) {
data_20t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# Fill in missing data with an estimated dataset from either manual dataset (if converted) and/or adjacent stations
all_swe <- snow_datafill(data_soi = data_20t10, data_id, normal_max, normal_min)
}
# Does the site have between 20-30 years of data? Don't add in any additional data and jsut calculcate normals from
if (numberofyears_80_raw >= 20 && numberofyears_80_raw <= 30) {
# DON'T Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columns
all_swe <- df_normal_time %>%
#dplyr::filter(wr %in% df_normal_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80$numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
length(all_swe$numberofyears_estimated_80) > 0 && unique(all_swe$numberofyears_estimated_80) >= 20 && unique(all_swe$numberofyears_estimated_80) <= 30
station
numberofyears_80 <- df_nt %>%
ungroup() %>%
dplyr::filter(id %in% station) %>%
dplyr::select(numberofyears_80_raw) %>%
unique()
df_normal_time <- df_nt %>%
dplyr::filter(id %in% station)
dfn_80 <- df_normal_80 %>%
dplyr::filter(id %in% station)
if (dim(numberofyears_80)[1] == 0) {
numberofyears_80_raw <- 0
} else {
numberofyears_80_raw <- numberofyears_80$numberofyears_80_raw
}
# =============================
# Fill in data depending on how many years of data there are available
# Is there less than 10 years of data?
if (numberofyears_80_raw < 10) {
data_0t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# ++++++++++++++++++++++++++++++++++++++++++++++
# Use function to check to see if there is manual site to extend data.
# For now, do not calculate a normal
# Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columms
all_swe <- data_0t10 %>%
dplyr::filter(wr %in% dfn_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# Does the station have between 10-20 years of data? If so, extend the dataset using 1) manual dataset (if converted), and 2) adjacent stations
if (numberofyears_80_raw >= 10 && numberofyears_80_raw < 20) {
data_20t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# Fill in missing data with an estimated dataset from either manual dataset (if converted) and/or adjacent stations
all_swe <- snow_datafill(data_soi = data_20t10, data_id, normal_max, normal_min)
}
# Does the site have between 20-30 years of data? Don't add in any additional data and jsut calculcate normals from
if (numberofyears_80_raw >= 20 && numberofyears_80_raw <= 30) {
# DON'T Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columns
all_swe <- df_normal_time %>%
#dplyr::filter(wr %in% df_normal_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80$numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
length(all_swe$numberofyears_estimated_80) > 0
all_swe
numberofyears_80_raw < 10
numberofyears_80_raw
# Does the station have between 10-20 years of data? If so, extend the dataset using 1) manual dataset (if converted), and 2) adjacent stations
if (numberofyears_80_raw >= 10 && numberofyears_80_raw < 20) {
data_20t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# Fill in missing data with an estimated dataset from either manual dataset (if converted) and/or adjacent stations
all_swe <- snow_datafill(data_soi = data_20t10, data_id, normal_max, normal_min)
}
data_20t10
data_soi = data_20t10
unique(data_soi$id) %in% bcsnowdata::snow_auto_location()$LOCATION_ID
# Was the site converted from a manual site to a ASWE site? Get the name without the P (P shows that the site is ASWE)
station_id_manual <- substring(unique(data_soi$id), 1, 4)
station_id_manual
# ==================
# Fill in any missing data with data interpolated from stations within 100km of the station using a normal ratio method
# Are there stations within 100 km?
# First, get the location of the station you are looking at
location_station <- bcsnowdata::snow_auto_location() %>%
dplyr::filter(LOCATION_ID %in% unique(data_soi$id))
location_station <- sf::st_as_sf(location_station)
# All other sites within the vicinity
location_all <- sf::st_as_sf(bcsnowdata::snow_auto_location()) %>%
dplyr::filter(!(LOCATION_ID %in% unique(data_soi$id)))
# 100 km buffer around the site
fr_buffer <- sf::st_buffer(location_station, dist = 1e5)
# Filter for those sites within a 100 km buffer
ASWE_100km <- sf::st_filter(location_all, fr_buffer)
# If there are ASWE sites within a 100 km radius, the, proceed with the normal ratio method to backfill missing data
if (length(unique(ASWE_100km$LOCATION_ID)) > 0) {
############## Get the weight of each station
# Get the data for the first station
stations_adj <- unique(ASWE_100km$LOCATION_ID)
# use function to return data estimated from normal ratio method using stations within 100 km of the station of interest
all_swe <- aswe_normalratio(data_soi, stations_adj, data_id, normal_max, normal_min)
} else {
# If there are no stations within a 100 km radius and 20-10 years of data, then do not calculate a normal.
all_swe <- data_soi %>%
dplyr::arrange(date_utc) %>%
dplyr::mutate(swe_fornormal = values_stats)  # make a new column that clearly shows the data to use for normal calculation. If there is 20-10 years of data, and no nearby stations to estimate data, no normal calculated
}
length(unique(ASWE_100km$LOCATION_ID)) > 0
############## Get the weight of each station
# Get the data for the first station
stations_adj <- unique(ASWE_100km$LOCATION_ID)
stations_adj
# use function to return data estimated from normal ratio method using stations within 100 km of the station of interest
all_swe <- aswe_normalratio(data_soi, stations_adj, data_id, normal_max, normal_min)
data_soi
unique(data_soi$id) %in% bcsnowdata::snow_auto_location()$LOCATION_ID
# Was the site converted from a manual site to a ASWE site? Get the name without the P (P shows that the site is ASWE)
station_id_manual <- substring(unique(data_soi$id), 1, 4)
# ==================
# Fill in any missing data with data interpolated from stations within 100km of the station using a normal ratio method
# Are there stations within 100 km?
# First, get the location of the station you are looking at
location_station <- bcsnowdata::snow_auto_location() %>%
dplyr::filter(LOCATION_ID %in% unique(data_soi$id))
location_station <- sf::st_as_sf(location_station)
# All other sites within the vicinity
location_all <- sf::st_as_sf(bcsnowdata::snow_auto_location()) %>%
dplyr::filter(!(LOCATION_ID %in% unique(data_soi$id)))
# 100 km buffer around the site
fr_buffer <- sf::st_buffer(location_station, dist = 1e5)
# Filter for those sites within a 100 km buffer
ASWE_100km <- sf::st_filter(location_all, fr_buffer)
# If there are ASWE sites within a 100 km radius, the, proceed with the normal ratio method to backfill missing data
if (length(unique(ASWE_100km$LOCATION_ID)) > 0) {
############## Get the weight of each station
# Get the data for the first station
stations_adj <- unique(ASWE_100km$LOCATION_ID)
# use function to return data estimated from normal ratio method using stations within 100 km of the station of interest
all_swe <- aswe_normalratio(data_soi, stations_adj, data_id, normal_max, normal_min)
} else {
# If there are no stations within a 100 km radius and 20-10 years of data, then do not calculate a normal.
all_swe <- data_soi %>%
dplyr::arrange(date_utc) %>%
dplyr::mutate(swe_fornormal = values_stats)  # make a new column that clearly shows the data to use for normal calculation. If there is 20-10 years of data, and no nearby stations to estimate data, no normal calculated
}
# use function to return data estimated from normal ratio method using stations within 100 km of the station of interest
all_swe <- aswe_normalratio(data_soi, stations_adj, data_id, normal_max, normal_min)
