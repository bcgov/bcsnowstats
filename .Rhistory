sites_first[sites_first$Basin == "Similkameen",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "G" & substring(all_sites, 1, 1) == "2"), collapse = ";")
sites_first[sites_first$Basin == "SouthCoast",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "A" & substring(all_sites, 1, 1) == "3"), collapse = ";")
sites_first[sites_first$Basin == "VancouverIsland",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "B" & substring(all_sites, 1, 1) == "3"), collapse = ";")
sites_first[sites_first$Basin == "CentralCoast",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "C" & substring(all_sites, 1, 1) == "3"), collapse = ";")
sites_first[sites_first$Basin == "Skagit",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "D" & substring(all_sites, 1, 1) == "3"), collapse = ";")
sites_first[sites_first$Basin == "Peace",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "A" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "SkeenaNass",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "B" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "Liard",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "C" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "Stikine",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "D" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "Northwest",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "E" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "HaidaGwaii",][2] <- paste(NA, collapse = ";")
# basin not on the map currenly - future
sites_first[sites_first$Basin == "Nicola_old",][2] <- paste(c("1C01",	"1C09",	"1C19",	"1C25",	"1C29",	"2F13",	"2F18",	"2F23",	"2F24"), collapse = ";")
sites_first[sites_first$Basin == "FraserPlateau",][2] <- paste(c("1C08", "1C22", "1C21"), collapse = ";")
sites_first[sites_first$Basin == "LillBridge",][2] <- paste(c("1C06", "1C39", "1C38P", "1C38", "1C40P", "1C40", "1C12P", "1C14P", "1C14", "1C37", "1C05P", "1C05", "1C18P", "1C28"), collapse = ";")
sites_first[sites_first$Basin == "Quesnel",][2] <- paste(c("1C33A", "1C13A", "1C17", "1C20P", "1C23", "1C41P"), collapse = ";")
sites_first[sites_first$Basin == "LowerThompson",][2] <- paste(c("1C32", "1C09A", "1C19", "1C25", "1C29", "1C29P", "1C01"), collapse = ";")
sites_first[sites_first$Basin == "Fraser",][2] <- paste(subset(all_sites, substring(all_sites, 1, 1) == "1"), collapse = ";")
sites_first[sites_first$Basin == "Province",][2] <- paste(all_sites, collapse = ";")
if (id == "All") {
sites_id <- sites_first
} else {
# find the basin within the dataframe and return the basin name
sites_id <- sites_first[sites_first$Station_ID_used %like% id, ] %>%
dplyr::select(Basin) %>%
dplyr::distinct(Basin)
}
# Subset by the basin you want
if (basin == "All"){
sites_final <- sites_id
} else {
sites_final <- sites_id %>%
dplyr::filter(Basin %in% basin)
}
return(sites_final)
}
# ===============
#' Return the name of all snow basins within BC
#' @export
#' @keywords snow basin names
#' @examples \dontrun{}
snow_basins <- function() {
return(c("UpperFraserWest", "UpperFraserEast", "Nechako", "MiddleFraser", "LowerFraser", "NorthThompson",
"SouthThompson", "UpperColumbia", "WestKootenay", "EastKootenay", "Okanagan", "Boundary", "Similkameen", "SouthCoast",
"VancouverIsland", "CentralCoast", "Skagit", "Peace", "SkeenaNass", "Stikine", "Liard", "Northwest", "HaidaGwaii",
"Nicola_old", "FraserPlateau", "LillBridge", "Quesnel", "LowerThompson", "Fraser", "Province"))
}
# ======================
#' Legal annotation for plots
#' @export
#' @keywords internal
#' @examples \dontrun{}
annotation <- function() {
paste0("<b>Users should use the information on this website with caution and at their own risk.</b>", "<br>",
"Reproduction and analysis of data published by the BC Ministry of Environment, including data collected by affilated partners (more information through <a href= 'https://www2.gov.bc.ca/gov/content/environment/air-land-water/water/water-science-data/water-data-tools/snow-survey-data'>Snow Survey Data)</a>",
"<br>", " has not been produced in affiliation with or with the endorsement of the Ministry of Environment.")
}
# =================
#' Colour palette
#' @export
#' @keywords internal
#' @examples \dontrun{}
colour_p <- function() {
# Colours from Sam Albers's map - March 2019
colour_hex <- c("#FFFFFF", "#E50000", "#E69800", "#FFD380", "#FFFF00",
"#AAFF01", "#00FEC4", "#01C5FF", "#0071FE", "#002573")
colours_v <- viridis::viridis(1000)
# normal
colour_norm <- "#482475FF"
# curent year
colour_curr <- "#43BE71FF"
# mean
colour_mean <- "#7AD151FF"
colour_palette <- list(colour_hex = colour_hex, colours_v = colours_v, colour_norm = colour_norm, colour_curr = colour_curr, colour_mean = colour_mean)
}
# Fill in NA values with 0 is there is no snow at the pillow
if (dim(df_tmp_1)[1] > 0) {
df_stat_fill <- fillNA0(data = df_tmp_1)
} else {
df_stat_fill <- df_tmp_1
}
data = df_stat_fill
data_id = "value"
# ===========
# Calculate statistics through function. return a table of statistics for each day of the year
# ===========
# Compile this as a cache to speed up the function?
df_stat <- snow_stats(data = df_stat_fill, data_id = "value", normal_min, normal_max, force)
# ===========
# if the statistics function returns data, further calculate statistics for the day you specify
# ===========
if (dim(df_stat)[1] > 1) {
df_tmp_2 <- dplyr::full_join(df_stat_fill, df_stat, by = c("id", "m_d"))
#Select the user defined time interval
if (survey_period == "latest") {
#Select the last day there is data for
latest_stats <- df_tmp_2[which(df_tmp_2$date_utc == max(df_tmp_2$date_utc)), ] #select the most recent day of data
} else if (survey_period == "All") {
# return all data with stats
latest_stats <- df_tmp_2
} else {
# Select the day and time from the entire data
latest_stats <- df_tmp_2 %>%
dplyr::filter(m_d %in% survey_period)
}
# Subset by years selected
if (get_year == "All") {
latest_stats_1 <- latest_stats
} else {
latest_stats_1 <- latest_stats %>%
dplyr::filter(wr %in% bcsnowdata::wtr_yr(as.Date(get_year, format = "%Y")))
}
# Calculate statistics for the day you want
latest_stats_day <- latest_stats_1 %>%
dplyr::mutate(data_range = paste0(as.Date(min(date_utc)), " to ", as.Date(max(date_utc)))) %>%
dplyr::mutate(percent_Q50 = round((mean_day / Q50 * 100), digits = 2)) %>%
dplyr::mutate(percent_mean = round((mean_day / swe_mean * 100), digits = 2)) %>%
dplyr::mutate(percent_normal_mean = ifelse(!is.na(normal_swe_mean), round((mean_day / normal_swe_mean * 100), digits = 2),
NA)) %>%
dplyr::mutate(percent_normal_median = ifelse(!is.na(normal_Q50), round((mean_day / normal_Q50 * 100), digits = 2),
NA)) %>%
dplyr::group_by(m_d) %>%
dplyr::filter(!is.na(mean_day)) %>%
#dplyr::mutate(prctile = round(ecdf(mean_day)(mean_day)*100, digits = 2)) %>%
#dplyr::mutate(prctile = round(percent_rank(mean_day)*100, digits = 2)) %>% # try calculating the percentile for a month-day across all years by percent_rank
dplyr::arrange(id, date_utc)
# Calculate the percentile by day - historic SWE in the dataframe
if (dim(latest_stats_day)[1] >= 1) { # make sure there is data within the data frame
#if(!is.null(latest_stats_day$historic_SWE[[1]]) && unique(latest_stats_day$NumberofYears) >5) { # run ecdf if there is sufficient historic data: > 5 years?
latest_stats_2 <- latest_stats_day %>%
dplyr::group_by(id, date_utc) %>%
dplyr::mutate(percentile = ifelse(unique(numberofyears, na.rm = TRUE) > 5,
round(purrr::map2_dbl(historic_swe, value, ~ecdf(.x$mean_SWE)(.y)) * 100, digits = 2), NA)) %>%
dplyr::select(-historic_swe) %>%
dplyr::arrange(id, date_utc)
# Return rank of current value
#latest_stats_2$current_rank_min <- lapply(rank(as.numeric(c(paste0(unlist(latest_stats_day$historic_SWE)), latest_stats_day$value)))[length(c(paste0(unlist(latest_stats_day$historic_SWE)), latest_stats_day$value))]
latest_stats_2$current_rank_min <- mapply(rank_min_function, latest_stats_day$historic_swe, latest_stats_day$value)
# Return rank of current value
#latest_stats_2$current_rank_max <- rank(-(as.numeric(c(paste0(unlist(latest_stats_day$historic_SWE)), latest_stats_day$value))))[length(c(paste0(unlist(latest_stats_day$historic_SWE)), latest_stats_day$value))]
latest_stats_2$current_rank_max <- mapply(rank_max_function, latest_stats_day$historic_swe, latest_stats_day$value)
# Get the number of days until the peak
latest_stats_2$daystopeak_mean <- as.Date(paste0(latest_stats_2$date_peak_mean, "-", bcsnowdata::wtr_yr(latest_stats_2$date_utc)), format = "%m-%d-%Y") - as.Date(latest_stats_2$date_utc)
# Get the number of days until the peak
latest_stats_2$daystopeak_median <- as.Date(paste0(latest_stats_2$date_peak_median, "-", bcsnowdata::wtr_yr(latest_stats_2$date_utc)), format = "%m-%d-%Y") - as.Date(latest_stats_2$date_utc)
} else {
latest_stats_2 <- latest_stats_day %>%
dplyr::group_by(id, m_d) %>%
dplyr::mutate(percentile = NA) %>%
dplyr::mutate(current_rank_min = NA, current_rank_max = NA) %>%
dplyr::mutate(daystopeak_mean = NA, daystopeak_median = NA) %>%
dplyr::select(-historic_swe) %>%
dplyr::arrange(id, date_utc)
}
} else { # if there is no statistics available for the site in question
latest_stats_2 <- data.frame()
}
# If there is no data within the dataframe
if (dim(latest_stats_2)[1] < 1) {
entry <- t(data.frame(c(as.character(stations), survey_period)))
colnames(entry) <- c("id", "m_d")
latest_stats_3 <- dplyr::bind_rows(latest_stats_2, as.data.frame(entry))
} else {
latest_stats_3 <- latest_stats_2
}
# Ensure that all of the columns have the right class, otherwise they will not unlist as a dataframe
stats_out <- dplyr::as_tibble(latest_stats_3)
aswe_get_stats <- function(stations, survey_period, get_year, normal_min, normal_max, force = FALSE) {
print(paste0("Calculating statistics for ", stations))
# Get data for the station in order to calculate statistics
df_tmp_raw <- bcsnowdata::get_aswe_databc(station_id = stations,
get_year = "All",
parameter = "swe",
timestep = "daily"
)
# ===========
# Preprocessing
# ===========
# get water year
df_tmp_raw$wr <- bcsnowdata::wtr_yr(dates = df_tmp_raw$date_utc)
# ==
# get the mean SWE by day, rather than choosing just the 14:00 measurement
# ==
df_tmp_1 <- df_tmp_raw %>%
dplyr::mutate(m_d = format.Date(date_utc, "%m-%d"))  %>%
dplyr::mutate(date_dmy = as.Date(date_utc, format = "%Y-%m-%d")) %>%
dplyr::mutate(mean_day = value)
# ========================
# Fill in any missing data that should be zero
# When there is no snow at a pillow the SWE value can dip negative. Negative values are automatically cut out by Aquarius threshold rules
# Condition is to fill in 0 if teh last non na value is 10 mm or less for the same water year
# ========================
# Fill in NA values with 0 is there is no snow at the pillow
if (dim(df_tmp_1)[1] > 0) {
df_stat_fill <- fillNA0(data = df_tmp_1)
} else {
df_stat_fill <- df_tmp_1
}
# ========================
# Manually override any incorrect sites with the right value ****
# ========================
# ===========
# Calculate statistics through function. return a table of statistics for each day of the year
# ===========
# Compile this as a cache to speed up the function?
df_stat <- snow_stats(data = df_stat_fill, data_id = "value", normal_min, normal_max, force)
# ===========
# if the statistics function returns data, further calculate statistics for the day you specify
# ===========
if (dim(df_stat)[1] > 1) {
df_tmp_2 <- dplyr::full_join(df_stat_fill, df_stat, by = c("id", "m_d"))
#Select the user defined time interval
if (survey_period == "latest") {
#Select the last day there is data for
latest_stats <- df_tmp_2[which(df_tmp_2$date_utc == max(df_tmp_2$date_utc)), ] #select the most recent day of data
} else if (survey_period == "All") {
# return all data with stats
latest_stats <- df_tmp_2
} else {
# Select the day and time from the entire data
latest_stats <- df_tmp_2 %>%
dplyr::filter(m_d %in% survey_period)
}
# Subset by years selected
if (get_year == "All") {
latest_stats_1 <- latest_stats
} else {
latest_stats_1 <- latest_stats %>%
dplyr::filter(wr %in% bcsnowdata::wtr_yr(as.Date(get_year, format = "%Y")))
}
# Calculate statistics for the day you want
latest_stats_day <- latest_stats_1 %>%
dplyr::mutate(data_range = paste0(as.Date(min(date_utc)), " to ", as.Date(max(date_utc)))) %>%
dplyr::mutate(percent_Q50 = round((mean_day / Q50 * 100), digits = 2)) %>%
dplyr::mutate(percent_mean = round((mean_day / swe_mean * 100), digits = 2)) %>%
dplyr::mutate(percent_normal_mean = ifelse(!is.na(normal_swe_mean), round((mean_day / normal_swe_mean * 100), digits = 2),
NA)) %>%
dplyr::mutate(percent_normal_median = ifelse(!is.na(normal_Q50), round((mean_day / normal_Q50 * 100), digits = 2),
NA)) %>%
dplyr::group_by(m_d) %>%
dplyr::filter(!is.na(mean_day)) %>%
#dplyr::mutate(prctile = round(ecdf(mean_day)(mean_day)*100, digits = 2)) %>%
#dplyr::mutate(prctile = round(percent_rank(mean_day)*100, digits = 2)) %>% # try calculating the percentile for a month-day across all years by percent_rank
dplyr::arrange(id, date_utc)
# Calculate the percentile by day - historic SWE in the dataframe
if (dim(latest_stats_day)[1] >= 1) { # make sure there is data within the data frame
#if(!is.null(latest_stats_day$historic_SWE[[1]]) && unique(latest_stats_day$NumberofYears) >5) { # run ecdf if there is sufficient historic data: > 5 years?
latest_stats_2 <- latest_stats_day %>%
dplyr::group_by(id, date_utc) %>%
dplyr::mutate(percentile = ifelse(unique(numberofyears, na.rm = TRUE) > 5,
round(purrr::map2_dbl(historic_swe, value, ~ecdf(.x$mean_SWE)(.y)) * 100, digits = 2), NA)) %>%
dplyr::select(-historic_swe) %>%
dplyr::arrange(id, date_utc)
# Return rank of current value
#latest_stats_2$current_rank_min <- lapply(rank(as.numeric(c(paste0(unlist(latest_stats_day$historic_SWE)), latest_stats_day$value)))[length(c(paste0(unlist(latest_stats_day$historic_SWE)), latest_stats_day$value))]
latest_stats_2$current_rank_min <- mapply(rank_min_function, latest_stats_day$historic_swe, latest_stats_day$value)
# Return rank of current value
#latest_stats_2$current_rank_max <- rank(-(as.numeric(c(paste0(unlist(latest_stats_day$historic_SWE)), latest_stats_day$value))))[length(c(paste0(unlist(latest_stats_day$historic_SWE)), latest_stats_day$value))]
latest_stats_2$current_rank_max <- mapply(rank_max_function, latest_stats_day$historic_swe, latest_stats_day$value)
# Get the number of days until the peak
latest_stats_2$daystopeak_mean <- as.Date(paste0(latest_stats_2$date_peak_mean, "-", bcsnowdata::wtr_yr(latest_stats_2$date_utc)), format = "%m-%d-%Y") - as.Date(latest_stats_2$date_utc)
# Get the number of days until the peak
latest_stats_2$daystopeak_median <- as.Date(paste0(latest_stats_2$date_peak_median, "-", bcsnowdata::wtr_yr(latest_stats_2$date_utc)), format = "%m-%d-%Y") - as.Date(latest_stats_2$date_utc)
} else {
latest_stats_2 <- latest_stats_day %>%
dplyr::group_by(id, m_d) %>%
dplyr::mutate(percentile = NA) %>%
dplyr::mutate(current_rank_min = NA, current_rank_max = NA) %>%
dplyr::mutate(daystopeak_mean = NA, daystopeak_median = NA) %>%
dplyr::select(-historic_swe) %>%
dplyr::arrange(id, date_utc)
}
} else { # if there is no statistics available for the site in question
latest_stats_2 <- data.frame()
}
# If there is no data within the dataframe
if (dim(latest_stats_2)[1] < 1) {
entry <- t(data.frame(c(as.character(stations), survey_period)))
colnames(entry) <- c("id", "m_d")
latest_stats_3 <- dplyr::bind_rows(latest_stats_2, as.data.frame(entry))
} else {
latest_stats_3 <- latest_stats_2
}
# Ensure that all of the columns have the right class, otherwise they will not unlist as a dataframe
stats_out <- dplyr::as_tibble(latest_stats_3)
stats_out
}
tims_start2 <- Sys.time()
test <- get_snow_stats(station_id = c("2F18P"),
survey_period = "All",
get_year = "All",
normal_min = 1991,
normal_max = 2020,
force = FALSE)
time2 <- tims_start2 - Sys.time()
time2
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
tims_start2 <- Sys.time()
test <- get_snow_stats(station_id = c("2F18P"),
survey_period = "All",
get_year = "All",
normal_min = 1991,
normal_max = 2020,
force = FALSE)
time2 <- tims_start2 - Sys.time()
time2
View(test)
tims_start2 <- Sys.time()
test <- get_snow_stats(station_id = bcsnowdata::snow_manual_location()$LOCATION_ID[5],
survey_period = "All",
get_year = "All",
normal_min = 1991,
normal_max = 2020,
force = FALSE) %>%
dplyr::arrange(date_utc, survey_period)
time2 <- tims_start2 - Sys.time()
time2
tims_start <- Sys.time()
test <- get_snow_stats(station_id = bcsnowdata::snow_auto_location()$LOCATION_ID[2],
survey_period = "All",
get_year = "2022",
normal_min = 1991,
normal_max = 2020,
force = FALSE)
time <- tims_start - Sys.time()
tims_start <- Sys.time()
test <- get_snow_stats(station_id = bcsnowdata::snow_auto_location()$LOCATION_ID[2],
survey_period = "All",
get_year = "2022",
normal_min = 1991,
normal_max = 2020,
force = TRUE)
time <- tims_start - Sys.time()
station_id = bcsnowdata::snow_auto_location()$LOCATION_ID[2]
survey_period = "All"
get_year = "2022"
station_id = bcsnowdata::snow_auto_location()$LOCATION_ID[2]
survey_period = "All"
get_year = "2022"
normal_min = 1991
normal_max = 2020
force = TRUE
# Check to see whether the station is a manual or automated station
id_aswe <- station_id[station_id %in% bcsnowdata::snow_auto_location()$LOCATION_ID]
id_manual <- station_id[station_id %in% bcsnowdata::snow_manual_location()$LOCATION_ID]
length(id_aswe) > 0
length(id_manual) > 0
station_id = id_aswe
# Current water year from the system time
current_wy <- bcsnowdata::wtr_yr(Sys.time())
# If the input is All, get the station list from the website for current sites
if (any(station_id[1] == "All")){
station_list <- bcsnowdata::snow_auto_location()$LOCATION_ID
} else {
station_list <- unique(station_id)
}
# convert the survey_period into the right format (in case the input format is incorrect)
if (survey_period == "01-Jan"){
survey_period <- "01-01"
} else if (survey_period == "01-Feb"){
survey_period <-  "02-01"
} else if (survey_period == "01-Mar"){
survey_period <-  "03-01"
} else if (survey_period == "01-Apr"){
survey_period <-  "04-01"
} else if (survey_period == "01-May"){
survey_period <-  "05-01"
} else if (survey_period == "15-May"){
survey_period <-  "05-15"
} else if (survey_period == "01-Jun"){
survey_period <-  "06-01"
} else if (survey_period == "15-Jun"){
survey_period <-  "06-15"
} else if (survey_period == "latest"){
survey_period <- "latest"
} else {
survey_period <- survey_period
}
# Getting data at this point truncates data returned! Get data within lapply loop
# use get_percentile function to calculate statistics for the dates and stations specified
list_stats <- lapply(station_list,
aswe_get_stats,
survey_period = survey_period,
get_year = get_year,
normal_min, normal_max, force)
# unfold the list you created
df_final_1 <- do.call(dplyr::bind_rows, list_stats)
# create an empty row for stations that did not return any data for the period specified
if (!isTRUE(all.equal(station_list, unique(df_final_1$id))) | !is.data.frame(df_final_1)) {
missing <- tibble::tibble(id = stations[!(stations %in% unique(df_final_1$id))])
df_final_2 <- dplyr::bind_rows(df_final_1, missing)
} else {
df_final_2 <- df_final_1
}
!isTRUE(all.equal(station_list, unique(df_final_1$id))) | !is.data.frame(df_final_1)
station_list
!isTRUE(all.equal(station_list, unique(df_final_1$id)))
unique(df_final_1$id)
all.equal(station_list, unique(df_final_1$id))
!isTRUE(all.equal(station_list, unique(df_final_1$id)))
!is.data.frame(df_final_1)
stations
missing <- tibble::tibble(id = station_list[!(station_list %in% unique(df_final_1$id))])
missing
df_final_2 <- dplyr::bind_rows(df_final_1, missing)
# create an empty row for stations that did not return any data for the period specified
if (!isTRUE(all.equal(station_list, unique(df_final_1$id))) | !is.data.frame(df_final_1)) {
missing <- tibble::tibble(id = station_list[!(station_list %in% unique(df_final_1$id))])
df_final_2 <- dplyr::bind_rows(df_final_1, missing)
} else {
df_final_2 <- df_final_1
}
library(bcsnowstats)
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
tims_start <- Sys.time()
test <- get_snow_stats(station_id = bcsnowdata::snow_auto_location()$LOCATION_ID[2],
survey_period = "All",
get_year = "2022",
normal_min = 1991,
normal_max = 2020,
force = TRUE)
time <- tims_start - Sys.time()
View(test)
tims_start <- Sys.time()
test <- get_snow_stats(station_id = bcsnowdata::snow_auto_location()$LOCATION_ID[5],
survey_period = "All",
get_year = "2022",
normal_min = 1991,
normal_max = 2020,
force = TRUE)
time <- tims_start - Sys.time()
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
# See package dependencies - how functions within the package relate to each other
#library(DependenciesGraphs)
#deps <- funDependencies("package:bcsnowstats", "generate_split_coded_comments")
#deps <- envirDependencies("package:bcsnowstats")
#plot(deps)
tims_start <- Sys.time()
test <- get_snow_stats(station_id = bcsnowdata::snow_auto_location()$LOCATION_ID[50],
survey_period = "All",
get_year = "2022",
normal_min = 1991,
normal_max = 2020,
force = TRUE)
time <- tims_start - Sys.time()
View(test)
tims_start2 <- Sys.time()
test <- get_snow_stats(station_id = bcsnowdata::snow_manual_location()$LOCATION_ID[50],
survey_period = "All",
get_year = "All",
normal_min = 1991,
normal_max = 2020,
force = FALSE) %>%
dplyr::arrange(date_utc, survey_period)
time2 <- tims_start2 - Sys.time()
View(test)
drive = "\\\\DRAIN.dmz\\Shared"
drive_G = "\\\\Backhoe\\s63101\\Watershare\\rfc"
drive_Q = "\\\\question.bcgov\\envwwwt\\rfc"
drive_R = "\\\\answer.bcgov\\envwww\\rfc"
#source(paste0(drive_G, "/R/projects/Snow_Interactive_Map/R/Snow_Packages_Scripts.R"))
ASWE_sites <- bcsnowdata::snow_auto_location()$LOCATION_ID
tims_start2 <- Sys.time()
plot_test <- plot_interactive_aswe(id = bcsnowdata::snow_auto_location()$LOCATION_ID[1],
save = FALSE)
time2 <- tims_start2 - Sys.time()
id = bcsnowdata::snow_auto_location()$LOCATION_ID[1]
save = FALSE
# Get statistics data for the site you are plotting
data_plot_1 <- get_swe(id)
id
id
# Get statistics data for the site you are plotting
data_plot_1 <- get_snow_stats(station_id = id,
survey_period = "All",
get_year = "All",
normal_min = 1991,
normal_max = 2020,
force = FALSE)
# Get statistics data for the site you are plotting
data_plot_1 <- get_snow_stats(station_id = id,
survey_period = "All",
get_year = "All",
normal_min = 1991,
normal_max = 2020,
force = TRUE)
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
drive = "\\\\DRAIN.dmz\\Shared"
drive_G = "\\\\Backhoe\\s63101\\Watershare\\rfc"
drive_Q = "\\\\question.bcgov\\envwwwt\\rfc"
drive_R = "\\\\answer.bcgov\\envwww\\rfc"
#source(paste0(drive_G, "/R/projects/Snow_Interactive_Map/R/Snow_Packages_Scripts.R"))
ASWE_sites <- bcsnowdata::snow_auto_location()$LOCATION_ID
tims_start2 <- Sys.time()
plot_test <- plot_interactive_aswe(id = bcsnowdata::snow_auto_location()$LOCATION_ID[1],
save = FALSE)
time2 <- tims_start2 - Sys.time()
plot_test
tims_start2 <- Sys.time()
plot_test <- plot_interactive_aswe(id = bcsnowdata::snow_auto_location()$LOCATION_ID[10],
save = FALSE)
time2 <- tims_start2 - Sys.time()
install.packages(c("brew", "cli", "doParallel", "foreach", "gower", "iterators", "quantreg", "rlang", "sf", "svglite", "units", "V8"))
install.packages(c("brew", "cli", "doParallel", "foreach", "gower", "iterators", "quantreg", "rlang", "sf", "svglite", "units", "V8"))
