ASWE_sites_active[6:7]
id = ASWE_sites_active[6:7]
save
path
# Get statistics data for the site you are plotting
data_plot_1 <- get_swe(id)
# Run function over all sites that you have
plots <- lapply(unique(id),
plot_function,
data_plot_1,
save,
path)
tims_start2 <- Sys.time()
plot_test <- plot_interactive_aswe(id = ASWE_sites_active[6:7],
save = TRUE,
path = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/Interactive_plots/"))
time_save <- tims_start2 - Sys.time()
tims_start2 <- Sys.time()
plot_test <- plot_interactive_aswe(id = ASWE_sites_active[8:10],
save = TRUE,
path = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/Interactive_plots/"))
time_save <- tims_start2 - Sys.time()
ASWE_sites_active[8:10]
tims_start2 <- Sys.time()
plot_test <- plot_interactive_aswe(id = ASWE_sites_active[8],
save = TRUE,
path = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/Interactive_plots/"))
time_save <- tims_start2 - Sys.time()
tims_start2 <- Sys.time()
plot_test <- plot_interactive_aswe(id = ASWE_sites_active[9],
save = TRUE,
path = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/Interactive_plots/"))
time_save <- tims_start2 - Sys.time()
#' Interactive plots - ASWE sites (plot delta SWE and climate plots)
#' @param path path that you want to save plots in
#' @param id station ID that you want to plot SWE for
#' @param save whether to save the plot and stats. Defaults to 'No', Options are also TRUE
#' @importFrom magrittr %>%
#' @importFrom data.table %like%
#' @importFrom dplyr bind_rows
#' @export
#' @keywords plot ASWE SWE
#' @examples \dontrun{}
plot_interactive_aswe <- function (path, id, save = "No") {
# Get statistics data for the site you are plotting
data_plot_1 <- get_swe(id)
# Run function over all sites that you have
plots <- lapply(unique(id),
plot_function,
data_plot_1,
save,
path)
}
tims_start2 <- Sys.time()
plot_test <- plot_interactive_aswe(id = ASWE_sites_active[9:10],
save = TRUE,
path = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/Interactive_plots/"))
time_save <- tims_start2 - Sys.time()
id = ASWE_sites_active[9:10]
save
path
# Get statistics data for the site you are plotting
data_plot_1 <- get_swe(id)
id
# Get statistics data for the site you are plotting
data_plot_1 <- get_snow_stats(station_id = id,
survey_period = "All",
get_year = "All",
normal_min = 1991,
normal_max = 2020,
force = FALSE)
id
station_id = id
survey_period = "All"
get_year = "All"
normal_min = 1991
normal_max = 2020
force = FALSE
aswe <- bcsnowdata::snow_auto_location()$LOCATION_ID
man <- bcsnowdata::snow_manual_location()$LOCATION_ID
if (any(station_id %in% c("aswe", "ASWE", "Aswe"))) {
id_aswe <- aswe
} else if (any(station_id %in% c("manual", "MANUAL", "Manual", "man"))) {
id_manual <- man
} else if (any(station_id %in% c("ALL", "all", "All"))) {
id_aswe <- aswe
id_manual <- man
} else {
# Check to see whether the station is a manual or automated station
id_aswe <- station_id[station_id %in% aswe]
id_manual <- station_id[station_id %in% man]
}
length(id_aswe) > 0
id_aswe
station_id = id_aswe
# Current water year from the system time
current_wy <- bcsnowdata::wtr_yr(Sys.time())
# If the input is All, get the station list from the website for current sites
if (any(station_id[1] %in% c("All", "all", "ALL")) || any(station_id[1] %in% c("aswe", "ASWE", "Aswe"))) {
station_list <- unique(bcsnowdata::snow_auto_location()$LOCATION_ID)
} else {
station_list <- unique(station_id)
}
# convert the survey_period into the right format (in case the input format is incorrect)
if (survey_period == "01-Jan"){
survey_period <- "01-01"
} else if (survey_period == "01-Feb"){
survey_period <-  "02-01"
} else if (survey_period == "01-Mar"){
survey_period <-  "03-01"
} else if (survey_period == "01-Apr"){
survey_period <-  "04-01"
} else if (survey_period == "01-May"){
survey_period <-  "05-01"
} else if (survey_period == "15-May"){
survey_period <-  "05-15"
} else if (survey_period == "01-Jun"){
survey_period <-  "06-01"
} else if (survey_period == "15-Jun"){
survey_period <-  "06-15"
} else if (survey_period == "latest"){
survey_period <- "latest"
} else {
survey_period <- survey_period
}
print("get data")
# Get data for the station in order to calculate statistics
df <- bcsnowdata::get_aswe_databc(station_id = station_list,
get_year = "All",
parameter = "swe",
timestep = "daily"
)
# Getting data at this point truncates data returned! Get data within lapply loop
# use get_percentile function to calculate statistics for the dates and stations specified
print("calculate stats")
df_final_1 <- aswe_get_stats(stations = station_list,
data_all = df,
survey_period = survey_period,
get_year = get_year,
normal_min, normal_max, force)
stations = station_list
data_all = df
print(paste0("Calculating statistics for ", stations))
"id" %in% colnames(data_all)
# Get data for the station in order to calculate statistics
df_tmp_raw <- data_all #%>%
# ===========
# Preprocessing
# ===========
# get water year
df_tmp_raw$wr <- bcsnowdata::wtr_yr(dates = df_tmp_raw$date_utc)
# ==
# get the mean SWE by day, rather than choosing just the 14:00 measurement
# ==
df_tmp_1 <- df_tmp_raw %>%
dplyr::mutate(m_d = format.Date(date_utc, "%m-%d"))  %>%
dplyr::mutate(date_dmy = as.Date(date_utc, format = "%Y-%m-%d")) %>%
dplyr::mutate(mean_day = value)
# Fill in NA values with 0 is there is no snow at the pillow
if (dim(df_tmp_1)[1] > 0) {
df_stat_fill <- fillNA0(data = df_tmp_1)
} else {
df_stat_fill <- df_tmp_1
}
data = df_stat_fill
View(data)
data_id = "value"
# Ensure that current water year is defined
current_wy <- bcsnowdata::wtr_yr(Sys.Date())
# compute historical stats - for each day of the year from historical stats (prior to this water year)
# get historic dataset - previous to this year
df_hist <- data %>%
dplyr::filter(wr < current_wy) %>% # Filter out the current year data prior to calculating statistics
dplyr::group_by(id, m_d) %>% # Group by the station ID and the month/day
dplyr::rename(values_stats = all_of(data_id)) %>% # The user can define what data to run the statistics on. Usually this is the daily mean
dplyr::filter(!is.na(values_stats)) # filter out missing data
# Calculate the statistics on the data you specify
df_stat <- do.call(data.frame,
list(dplyr::summarise(df_hist, min = min(values_stats, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, swe_mean = mean(values_stats, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q5 = quantile(values_stats, 0.05, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q10 = quantile(values_stats, 0.1, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q25 = quantile(values_stats, 0.25, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q50 = quantile(values_stats, 0.5,na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q75 = quantile(values_stats,0.75, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q90 = quantile(values_stats,0.90, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, max = max(values_stats, na.rm = TRUE), .groups = "keep"))) %>%
dplyr::select(-m_d.1, -m_d.2, -m_d.3, -m_d.4, -m_d.5, -m_d.6, -m_d.7, -m_d.8) %>%
dplyr::select(-id.1, -id.2, -id.3, -id.4, -id.5, -id.6, -id.7, -id.8)
# -------------------------------
# Get the max, min dates, as well as the data range and number of years
# Number of years. Implement threshold
df_time <- data %>%
dplyr::rename(values_stats = all_of(data_id)) %>% # The user can define what data to run the statistics on. Usually this is the daily mean
dplyr::ungroup() %>%
dplyr::filter(!is.na(values_stats)) # filter out NA values to get more accurate start and end dates
# Get the years that have >80% of data coverage during the snow accumulation period! Oct 1 - June 30
df_time_2 <- df_time %>%
dplyr::arrange(id, date_utc) %>%
dplyr::ungroup() %>%
dplyr::group_by(id, wr) %>%
dplyr::filter(m_d >= "10-01" | m_d <= "06-30") %>%
dplyr::mutate(percent_available = length(values_stats) / as.numeric(abs(difftime(as.POSIXct("2018-10-01"), as.POSIXct("2019-06-30"), units = "days")))*100) %>%
dplyr::select(id, wr, percent_available) %>%
unique() %>%
dplyr::filter(wr == bcsnowdata::wtr_yr(Sys.Date()) | percent_available >= 50)
daterange <- df_time_2 %>%
dplyr::ungroup() %>%
dplyr::group_by(id) %>%
dplyr::summarize(maxdate = max(wr), mindate = min(wr), .groups = "keep") %>%
dplyr::mutate(data_range = (paste0(mindate, " to ", maxdate)))
numberyears <- df_time_2 %>%
dplyr::ungroup() %>%
dplyr::group_by(id) %>%
dplyr::filter(wr != bcsnowdata::wtr_yr(Sys.Date())) %>%
unique() %>%
dplyr::mutate(numberofyears = length(percent_available)) %>%
dplyr::select(id, numberofyears) %>%
unique()
# Bind stats regarding the number of years available together
df_range <- dplyr::full_join(daterange, numberyears, by = c("id"))
# Bind to the statistics
df_stat_date <- dplyr::full_join(df_stat, df_range, by = c("id"))
# get the day of the max and min!!
min_date <- df_hist %>%
dplyr::ungroup() %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.min(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_min_utc = date_utc)
# Get the date of the max value for the date
max_date <- df_hist %>%
dplyr::ungroup() %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.max(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_max_utc = date_utc)
# append to data
dates <- dplyr::full_join(min_date, max_date, by = c("id", "m_d"))
df_stat_1 <- dplyr::full_join(df_stat_date, dates, by = c("id", "m_d"))
### Calculate normals using function.
# Function contains all of the thresholds, etc that make it
df_normals_1 <- SWE_normals(data = df_time, normal_max, normal_min, force)
data = df_time
aswe <- bcsnowdata::snow_auto_location()$LOCATION_ID
manual <- bcsnowdata::snow_manual_location()$LOCATION_ID
# if the user input data as a station name (i.e., the function is being used as a stand alone function), get the data for the station
if (all(data %in% aswe)) {
data_norm <- bcsnowdata::get_aswe_databc(
station_id = data,
get_year = "All",
parameter = "swe",
timestep = "daily") %>%
dplyr::rename("values_stats" = value)
} else if (all(data %in% manual)) {
data_norm <- bcsnowdata::get_manual_swe(
station_id = data,
get_year = "All",
survey_period = "All")
} else {
data_norm <- data
if ("value" %in% colnames(data_norm)) {
data_norm <- data_norm %>%
dplyr::rename("values_stats" = value)
}
}
id <- unique(data_norm$id)
id
dim(data_norm)[1] == 0
any(id %in% aswe)
# filter data for ASWE sites
data_swe <- data_norm %>%
dplyr::filter(id %in% aswe)
data = data_swe
data_id = "values_stats"
data_dir <- function() {
if (R.Version()$major >= 4) {
getOption("bcsnowstats.data_dir", default = tools::R_user_dir("bcsnowstats", "cache"))
} else {
getOption("bcsnowstats.data_dir", default = rappdirs::user_cache_dir("bcsnowstats"))
}
}
show_cached_files <- function() {
file.path(list.files(data_dir(), full.names = TRUE))
}
check_write_to_data_dir <- function(dir, ask) {
if (ask) {
ans <- gtools::ask(paste("bcsnowstats would like to store this layer in the directory:",
dir, "Is that okay?", sep = "\n"))
if (!(ans %in% c("Yes", "YES", "yes", "y"))) stop("Exiting...", call. = FALSE)
}
if (!dir.exists(dir)) {
message("Creating directory to hold bcsnowstats data at ", dir)
dir.create(dir, showWarnings = FALSE, recursive = TRUE)
} else {
message("Saving to bcsnowstats data directory at ", dir)
}
}
# Check to ensure that the ASWE archived data has been cached on the user's computer and is up to date
fname <- paste0(unique(data$parameter), "_norm_archive.rds")
dir <- data_dir()
fpath <- file.path(dir, fname)
any(!file.exists(fpath)) | force
# Get the previously cached data
df_normals_initial <- readRDS(fpath)
# Check to ensure that the data contains statistics with the right normal range. Filter for the range you are looking for
check <- df_normals_initial %>%
dplyr::filter(initial_normal_range == paste0(normal_min, " to ", normal_max)) %>%
dplyr::filter(id %in% unique(data$id))
dim(check)[1] < 1 | !(all(unique(check$id) %in% unique(data$id)))
# Put data into right format using the data_massage function
data_m <- data_massage(data)
if ("swe_mean" %in% colnames(data_m)) {
data_id <- "swe_mean" # reassign the data_ID value
}
# Filter the data by the normal span that you specify
df_normal_time <- data_m %>%
dplyr::filter(wr <= normal_max, wr >= normal_min) %>% # Filter by the normal dates that you specify
dplyr::group_by(id, m_d) %>%
dplyr::rename(values_stats = all_of(data_id))
View(df_normal_time)
# ++++++++++++++++++++++ thresholds
# Check to see whether there is sufficient data to calculate a normal.
# The WMO recommends only calculating a normal for stations that have 80% of the data available
# Firstly, just show the amount of data available for the normal period
# Number of years with 80% or great of the data available.
# Only count the data between Oct-June - doesn't matter if the snow data is missing in summer - 273 days in snow accumulation/melt season
# Only for ASWE
df_normal_80 <- df_normal_time %>%
dplyr::filter(!is.na(values_stats)) %>% # # filter out missing data
dplyr::ungroup() %>%
dplyr::group_by(id, wr) %>%
dplyr::filter(lubridate::month(as.Date(m_d, format = "%m-%d")) <= 6 || lubridate::month(as.Date(m_d, format = "%m-%d")) >= 10) %>% # get only the snow accumulation and melt season
dplyr::mutate(percent_available = length(values_stats) / length(seq(as.Date("2020-10-01"), as.Date("2021-06-30"), by = "day")) * 100) %>%
dplyr::select(id, wr, percent_available) %>%
unique() %>%
dplyr::filter(percent_available >= 80) # filter by the 80% WMO threshold
# Get the number of years within the normal range with >= 80% data coverage within a specific year
ny_80 <- df_normal_80 %>%
dplyr::group_by(id) %>%
dplyr::summarize(numberofyears_80_raw = n())
# Add the number of years with 80% of data to the dataframe
df_nt <- df_normal_time %>%
dplyr::full_join(ny_80)
normals <- lapply(unique(data$id),
calc_norm,
df_nt,
df_normal_80, normal_max = normal_max, normal_min = normal_min)
df_nt
ny_80
ny_80
all(is.na(ny_80$numberofyears_80_raw)
)
if (all(is.na(ny_80$numberofyears_80_raw))) {
ny_80$numberofyears_80_raw == 0
}
ny_80$numberofyears_80_raw
ny_80
df_normal_80
df_normal_time
# Add the number of years with 80% of data to the dataframe
df_nt <- df_normal_time %>%
dplyr::full_join(ny_80)
# Add the number of years with 80% of data to the dataframe
df_nt <- df_normal_time %>%
dplyr::full_join(ny_80) %>%
dplyr::mutate(numberofyears_80_raw = ifelse(is.na(), 0, numberofyears_80_raw))
df_nt <- df_normal_time %>%
dplyr::full_join(ny_80) %>%
dplyr::mutate(numberofyears_80_raw = ifelse(is.na(numberofyears_80_raw), 0, numberofyears_80_raw))
View(df_nt)
normals <- lapply(unique(data$id),
calc_norm,
df_nt,
df_normal_80, normal_max = normal_max, normal_min = normal_min)
# Function for defining whether to fill in data and calculate normals. To be run station by station
calc_norm <- function(station, df_nt, df_normal_80, normal_max, normal_min) {
numberofyears_80 <- df_nt %>%
ungroup() %>%
dplyr::filter(id %in% station) %>%
dplyr::select(numberofyears_80_raw) %>%
unique()
df_normal_time <- df_nt %>%
dplyr::filter(id %in% station)
dfn_80 <- df_normal_80 %>%
dplyr::filter(id %in% station)
if (dim(numberofyears_80)[1] == 0) {
numberofyears_80_raw <- 0
} else {
numberofyears_80_raw <- numberofyears_80$numberofyears_80_raw
}
# =============================
# Fill in data depending on how many years of data there are available
# Is there less than 10 years of data?
if (numberofyears_80_raw < 10) {
data_0t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# ++++++++++++++++++++++++++++++++++++++++++++++
# Use function to check to see if there is manual site to extend data.
# For now, do not calculate a normal
# Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columms
all_swe <- data_0t10 %>%
dplyr::filter(wr %in% dfn_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# Does the station have between 10-20 years of data? If so, extend the dataset using 1) manual dataset (if converted), and 2) adjacent stations
if (numberofyears_80_raw >= 10 && numberofyears_80_raw < 20) {
data_20t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# Fill in missing data with an estimated dataset from either manual dataset (if converted) and/or adjacent stations
all_swe <- snow_datafill(data_soi = data_20t10, data_id, normal_max, normal_min)
}
# Does the site have between 20-30 years of data? Don't add in any additional data and jsut calculcate normals from
if (numberofyears_80_raw >= 20 && numberofyears_80_raw <= 30) {
# DON'T Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columns
all_swe <- df_normal_time %>%
#dplyr::filter(wr %in% df_normal_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80$numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# End of data filling according to thresholds
# ==============================
# Calculate normals. Only calculate normal if there is sufficient data
if (length(all_swe$numberofyears_estimated_80) > 0 && unique(all_swe$numberofyears_estimated_80) >= 20 && unique(all_swe$numberofyears_estimated_80) <= 30) {
all_swe_1 <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::filter(!is.na(swe_fornormal))
# Calculate the normal statistics for each day of the year
df_normals <- do.call(data.frame,
list(dplyr::summarise(all_swe_1, normal_minimum = min(swe_fornormal, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_swe_mean = mean(swe_fornormal, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q5 = quantile(swe_fornormal, 0.05, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q10 = quantile(swe_fornormal, 0.1, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q25 = quantile(swe_fornormal, 0.25, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q50 = quantile(swe_fornormal, 0.5, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q75 = quantile(swe_fornormal, 0.75, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q90 = quantile(swe_fornormal, 0.90, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_maximum = max(swe_fornormal, na.rm = TRUE), .groups = "keep"))) %>%
dplyr::select(-m_d.1, -m_d.2, -m_d.3, -m_d.4, -m_d.5, -m_d.6, -m_d.7, -m_d.8) %>%
dplyr::select(-id.1, -id.2, -id.3, -id.4, -id.5, -id.6, -id.7, -id.8) %>%
#dplyr::mutate(Data_Range_normal = (paste0(round(normal_minimum, digits = 0), ' to ', round(normal_maximum, digits = 0)))) %>%
dplyr::mutate(data_range_normal = (paste0(min(lubridate::year(all_swe$date_utc), na.rm = TRUE), " to ", max(lubridate::year(all_swe$date_utc), na.rm = TRUE)))) %>%
dplyr::mutate(normal_datarange_estimated = unique(all_swe$numberofyears_estimated_80, na.rm = TRUE)[!is.na(unique(all_swe$numberofyears_estimated_80, na.rm = TRUE))]) %>%
dplyr::mutate(normal_datarange_raw = unique(all_swe$numberofyears_80_raw, na.rm = TRUE)[!is.na(unique(all_swe$numberofyears_80_raw, na.rm = TRUE))])
# get the day of the max and min!! Use only 'real', non estimated data
min_date <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.min(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_min_normal_utc = date_utc)
max_date <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.max(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_max_normal_utc = date_utc)
# append to data
dates <- dplyr::full_join(min_date, max_date, by = c("id", "m_d"))
df_normals_out <- dplyr::full_join(df_normals, dates, by = c("id", "m_d")) %>%
dplyr::mutate(initial_normal_range = paste0(normal_min, " to ", normal_max))
# Smooth all the statistics by the 5 -day average?
# If there is less than 10 years of data available even after trying adjacent sites, return
} else {
df_normals_out <- data.frame("id"  = unique(data$id),
"m_d" = NA,
"normal_minimum" = NA,
"normal_swe_mean" = NA,
"normal_Q5" = NA,
"normal_Q10" = NA,
"normal_Q25"  = NA,
"normal_Q50" = NA,
"normal_Q75" = NA,
"normal_Q90" = NA,
"normal_maximum" = NA,
"data_range_normal" = NA,
"initial_normal_range" = paste0(normal_min, " to ", normal_max),
"normal_datarange_estimated" = NA,
"normal_datarange_raw" = NA,
"date_min_normal_utc" = NA,
"date_max_normal_utc"  = NA)
}
df_normals_out
}
normals <- lapply(unique(data$id),
calc_norm,
df_nt,
df_normal_80, normal_max = normal_max, normal_min = normal_min)
df_normals_out <- do.call(rbind, normals)
View(df_normals_out)
drive = "\\\\DRAIN.dmz\\Shared"
drive_G = "\\\\Backhoe\\s63101\\Watershare\\rfc"
drive_Q = "\\\\question.bcgov\\envwwwt\\rfc"
drive_R = "\\\\answer.bcgov\\envwww\\rfc"
ASWE_sites <- bcsnowdata::snow_auto_location() %>%
dplyr::filter(STATUS == "Active")
ASWE_sites_active <- ASWE_sites$LOCATION_ID
id_test = bcsnowdata::snow_auto_location()$LOCATION_ID[10]
tims_start2 <- Sys.time()
plot_test <- plot_interactive_aswe(id = ASWE_sites_active[9:10],
save = TRUE,
path = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/Interactive_plots/"))
time_save <- tims_start2 - Sys.time()
tims_start2 <- Sys.time()
plot_test <- plot_interactive_aswe(id = ASWE_sites_active[11:20],
save = TRUE,
path = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/Interactive_plots/"))
time_save <- tims_start2 - Sys.time()
time_save
length(ASWE_sites_active)
tims_start2 <- Sys.time()
plot_test <- plot_interactive_aswe(id = ASWE_sites_active[21:length(ASWE_sites_active)],
save = TRUE,
path = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/Interactive_plots/"))
time_save <- tims_start2 - Sys.time()
