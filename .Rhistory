df <- list()
for (i in 1:length(starts)) {
firstNAseq <- starts[i]
if (i != length(starts)) {
temp <- df_na_3[starts[i]:(starts[i + 1] - 1), ]
} else {
temp <- df_na_3[starts[i]:dim(df_na_3)[1], ]
}
date_i <- df_na_3[starts[i], ]$date_dmy
temp$Seq_3 <- temp$date_dmy - date_i
# If the day after the first non-na value immediately follows it, fill in 0
if (dim(temp)[1] == 1) {
# If there is only one instance, the data out is the same as the data in
temp_out <- temp
} else if (temp$Seq_3[2] == 1) {
# Filter so that you are taking only 30 days after the last non-NA value - not extending a 0 past 20 days
temp <- temp %>%
dplyr::filter(Seq_3 <= 30)# %>%
# dplyr::mutate(flag = 1:n())
# indicate that the 0 is filled in via the code
temp[is.na(temp$mean_day), ]$code <- "0Filledin"
# fill in the NA values
temp[is.na(temp$mean_day), ]$mean_day <- 0
temp_out <- temp
#dplyr::select(-Seq, -flag, -Seq_2, -Seq_3)
} else if (temp$Seq_3[2] > 1) {
temp_out <- NULL
}
df[[i]] <- temp_out
}
# Unwrap the list you just made
unwrap <- do.call(dplyr::bind_rows, df) %>%
dplyr::select(-Seq, -flag, -Seq_2, -Seq_3)
# Replace data within the initial dataset with dataset you just created
# Remove the entries that exist within the new dataframe in the initial dataset
data_tomerge <- data[!(data$date_dmy %in% unwrap$date_dmy), ]
data_final <- dplyr::full_join(data_tomerge, unwrap, by = c("date_utc", "station_id", "value", "code", "variable", "station_name", "wr", "m_d", "date_dmy", "mean_day")) %>%
dplyr::arrange(date_utc)
} else if (length(first_0) == 0) {
# If there is no data to fill in, simply return the data that was input into the function
data_final <- data
}
} else if (length(NAvalues) == 0) {
data_final <- data
}
return(data_final)
}
# ===========================
#' Function for rounding all digits within a data frame to specific number of digits
#' @param x data frame
#' @param digits number of digits you want to round data to
#' @keywords internal
#' @examples \dontrun{}
round_df <- function(x, digits) {
# round all numeric variables
# x: data frame
# digits: number of digits to round
numeric_columns <- sapply(x, is.numeric)
x[numeric_columns] <-  round(x[numeric_columns], digits)
x
}
# =====================
#' Function for returning elevation by station
#' @export
#' @keywords snow elevations
#' @examples \dontrun{}
elevation <- function() {
manual_data <- bcsnowdata::snow_manual_location() %>%
#dplyr::filter(!is.na(SWE_mm)) %>%
dplyr::arrange(LOCATION_ID) %>%
dplyr::select(LOCATION_NAME, LOCATION_ID, ELEVATION) %>%
dplyr::rename(Elevation = ELEVATION) %>%
dplyr::rename(Name = LOCATION_NAME) %>%
dplyr::rename(Station_ID = LOCATION_ID) %>%
dplyr::distinct(Station_ID, .keep_all = TRUE)
auto <- bcsnowdata::snow_auto_location() %>%
dplyr::rename(Station_ID = LOCATION_ID) %>%
dplyr::select(Station_ID, ELEVATION, LOCATION_NAME, geometry) %>%
dplyr::rename(Elevation = ELEVATION, Name = LOCATION_NAME)
stations_el <- bind_rows(auto, manual_data)
}
# =====================
#' Function for assigning a basin name to a station ID
#' @param id id of station you are assigning to a basin
#' @param basin name of basin
#' @param exceptions stations to skip
#' @importFrom data.table %like%
#' @export
#' @keywords snow site basin name
#' @examples \dontrun{}
basin_name <- function(id = "All", basin = "All", exceptions = NULL) {
# get all of the sites within the archive
all_sites <- unique(c(bcsnowdata::snow_auto_location()$LOCATION_ID, bcsnowdata::snow_manual_location()$LOCATION_ID))
# Apply exceptions - remove sites that should be removed
all_sites <-  all_sites[!all_sites %in% exceptions]
# associate basins by the ID number
basins_all <- snow_basins()
sites_first <- data.frame(Basin = basins_all, Station_ID_used = 2)
sites_first[sites_first$Basin == "UpperFraserWest",][2] <- paste(c("1A04", "1A07", "1A12", "1A16", "1A23", "1A24", "1A12P"), collapse = ";")
sites_first[sites_first$Basin == "UpperFraserEast",][2] <- paste(c("1A01P", "1A02P", "1A03P", "1A05P", "1A14P", "1A15P", "1A17P", "1A19P",
"1A05","1A06", "1A06A","1A08", "1A09", "1A10",
"1A11", "1A13", "1A15", "1A18", "1A20", "1A21", "1A22"), collapse = ";")
sites_first[sites_first$Basin == "Nechako",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "B"& substring(all_sites, 1, 1) == "1"), collapse = ";")
sites_first[sites_first$Basin == "MiddleFraser",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "C" & substring(all_sites, 1, 1) == "1"), collapse = ";")
sites_first[sites_first$Basin == "LowerFraser",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "D" & substring(all_sites, 1, 1) == "1"), collapse = ";")
sites_first[sites_first$Basin == "NorthThompson",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "E" & substring(all_sites, 1, 1) == "1"), collapse = ";")
sites_first[sites_first$Basin == "SouthThompson",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "F" & substring(all_sites, 1, 1) == "1"), collapse = ";")
sites_first[sites_first$Basin == "UpperColumbia",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "A" & substring(all_sites, 1, 1) == "2"), collapse = ";")
sites_first[sites_first$Basin == "WestKootenay",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) %in% c("D","B") & substring(all_sites, 1, 1) == "2"), collapse = ";")
sites_first[sites_first$Basin == "EastKootenay",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "C" & substring(all_sites, 1, 1) == "2"), collapse = ";")
sites_first[sites_first$Basin == "Okanagan",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "F" & substring(all_sites, 1, 1) == "2"), collapse = ";")
sites_first[sites_first$Basin == "Boundary",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "E" & substring(all_sites, 1, 1) == "2"), collapse = ";")
sites_first[sites_first$Basin == "Similkameen",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "G" & substring(all_sites, 1, 1) == "2"), collapse = ";")
sites_first[sites_first$Basin == "SouthCoast",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "A" & substring(all_sites, 1, 1) == "3"), collapse = ";")
sites_first[sites_first$Basin == "VancouverIsland",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "B" & substring(all_sites, 1, 1) == "3"), collapse = ";")
sites_first[sites_first$Basin == "CentralCoast",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "C" & substring(all_sites, 1, 1) == "3"), collapse = ";")
sites_first[sites_first$Basin == "Skagit",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "D" & substring(all_sites, 1, 1) == "3"), collapse = ";")
sites_first[sites_first$Basin == "Peace",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "A" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "SkeenaNass",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "B" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "Liard",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "C" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "Stikine",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "D" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "Northwest",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "E" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "HaidaGwaii",][2] <- paste(NA, collapse = ";")
# basin not on the map currenly - future
sites_first[sites_first$Basin == "Nicola_old",][2] <- paste(c("1C01",	"1C09",	"1C19",	"1C25",	"1C29",	"2F13",	"2F18",	"2F23",	"2F24"), collapse = ";")
sites_first[sites_first$Basin == "FraserPlateau",][2] <- paste(c("1C08", "1C22", "1C21"), collapse = ";")
sites_first[sites_first$Basin == "LillBridge",][2] <- paste(c("1C06", "1C39", "1C38P", "1C38", "1C40P", "1C40", "1C12P", "1C14P", "1C14", "1C37", "1C05P", "1C05", "1C18P", "1C28"), collapse = ";")
sites_first[sites_first$Basin == "Quesnel",][2] <- paste(c("1C33A", "1C13A", "1C17", "1C20P", "1C23", "1C41P"), collapse = ";")
sites_first[sites_first$Basin == "LowerThompson",][2] <- paste(c("1C32", "1C09A", "1C19", "1C25", "1C29", "1C29P", "1C01"), collapse = ";")
sites_first[sites_first$Basin == "Fraser",][2] <- paste(subset(all_sites, substring(all_sites, 1, 1) == "1"), collapse = ";")
sites_first[sites_first$Basin == "Province",][2] <- paste(all_sites, collapse = ";")
if (id == "All") {
sites_id <- sites_first
} else {
# find the basin within the dataframe and return the basin name
sites_id <- sites_first[sites_first$Station_ID_used %like% id, ] %>%
dplyr::select(Basin) %>%
dplyr::distinct(Basin)
}
# Subset by the basin you want
if (basin == "All"){
sites_final <- sites_id
} else {
sites_final <- sites_id %>%
dplyr::filter(Basin %in% basin)
}
return(sites_final)
}
# ===============
#' Return the name of all snow basins within BC
#' @export
#' @keywords snow basin names
#' @examples \dontrun{}
snow_basins <- function() {
return(c("UpperFraserWest", "UpperFraserEast", "Nechako", "MiddleFraser", "LowerFraser", "NorthThompson",
"SouthThompson", "UpperColumbia", "WestKootenay", "EastKootenay", "Okanagan", "Boundary", "Similkameen", "SouthCoast",
"VancouverIsland", "CentralCoast", "Skagit", "Peace", "SkeenaNass", "Stikine", "Liard", "Northwest", "HaidaGwaii",
"Nicola_old", "FraserPlateau", "LillBridge", "Quesnel", "LowerThompson", "Fraser", "Province"))
}
# ======================
#' Legal annotation for plots
#' @export
#' @keywords internal
#' @examples \dontrun{}
annotation <- function() {
paste0("<b>Users should use the information on this website with caution and at their own risk.</b>", "<br>",
"Reproduction and analysis of data published by the BC Ministry of Environment, including data collected by affilated partners (more information through <a href= 'https://www2.gov.bc.ca/gov/content/environment/air-land-water/water/water-science-data/water-data-tools/snow-survey-data'>Snow Survey Data)</a>",
"<br>", " has not been produced in affiliation with or with the endorsement of the Ministry of Environment.")
}
# =================
#' Colour palette
#' @export
#' @keywords internal
#' @examples \dontrun{}
colour_p <- function() {
# Colours from Sam Albers's map - March 2019
colour_hex <- c("#FFFFFF", "#E50000", "#E69800", "#FFD380", "#FFFF00",
"#AAFF01", "#00FEC4", "#01C5FF", "#0071FE", "#002573")
colours_v <- viridis::viridis(1000)
# normal
colour_norm <- "#482475FF"
# curent year
colour_curr <- "#43BE71FF"
# mean
colour_mean <- "#7AD151FF"
colour_palette <- list(colour_hex = colour_hex, colours_v = colours_v, colour_norm = colour_norm, colour_curr = colour_curr, colour_mean = colour_mean)
}
# Fill in NA values with 0 is there is no snow at the pillow
if (dim(df_tmp_1)[1] > 0) {
df_stat_fill <- fillNA0(data = df_tmp_1)
} else {
df_stat_fill <- df_tmp_1
}
# ===========
# Calculate statistics through function. return a table of statistics for each day of the year
# ===========
# Compile this as a cache to speed up the function?
df_stat <- snow_stats(data = df_stat_fill, data_id = "value", normal_min, normal_max, force)
data = df_stat_fill
data_id = "value"
# Ensure that current water year is defined
current_wy <- bcsnowdata::wtr_yr(Sys.Date())
# compute historical stats - for each day of the year from historical stats (prior to this water year)
# get historic dataset - previous to this year
df_hist <- data %>%
dplyr::filter(wr < current_wy) %>% # Filter out the current year data prior to calculating statistics
dplyr::group_by(id, m_d) %>% # Group by the station ID and the month/day
dplyr::rename(values_stats = all_of(data_id)) %>% # The user can define what data to run the statistics on. Usually this is the daily mean
dplyr::filter(!is.na(values_stats)) # filter out missing data
# Calculate the statistics on the data you specify
df_stat <- do.call(data.frame,
list(dplyr::summarise(df_hist, min = min(values_stats, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, swe_mean = mean(values_stats, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q5 = quantile(values_stats, 0.05, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q10 = quantile(values_stats, 0.1, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q25 = quantile(values_stats, 0.25, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q50 = quantile(values_stats, 0.5,na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q75 = quantile(values_stats,0.75, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q90 = quantile(values_stats,0.90, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, max = max(values_stats, na.rm = TRUE), .groups = "keep"))) %>%
dplyr::select(-m_d.1, -m_d.2, -m_d.3, -m_d.4, -m_d.5, -m_d.6, -m_d.7, -m_d.8) %>%
dplyr::select(-id.1, -id.2, -id.3, -id.4, -id.5, -id.6, -id.7, -id.8)
# -------------------------------
# Get the max, min dates, as well as the data range and number of years
# Number of years. Implement threshold
df_time <- data %>%
dplyr::rename(values_stats = all_of(data_id)) %>% # The user can define what data to run the statistics on. Usually this is the daily mean
dplyr::ungroup() %>%
dplyr::filter(!is.na(values_stats)) # filter out NA values to get more accurate start and end dates
# Get the years that have >80% of data coverage during the snow accumulation period! Oct 1 - June 30
df_time_2 <- df_time %>%
dplyr::arrange(id, date_utc) %>%
dplyr::ungroup() %>%
dplyr::group_by(id, wr) %>%
dplyr::filter(m_d >= "10-01" | m_d <= "06-30") %>%
dplyr::mutate(percent_available = length(values_stats) / as.numeric(abs(difftime(as.POSIXct("2018-10-01"), as.POSIXct("2019-06-30"), units = "days")))*100) %>%
dplyr::select(id, wr, percent_available) %>%
unique() %>%
dplyr::filter(wr == bcsnowdata::wtr_yr(Sys.Date()) | percent_available >= 50)
daterange <- df_time_2 %>%
dplyr::ungroup() %>%
dplyr::group_by(id) %>%
dplyr::summarize(maxdate = max(wr), mindate = min(wr), .groups = "keep") %>%
dplyr::mutate(data_range = (paste0(mindate, " to ", maxdate)))
numberyears <- df_time_2 %>%
dplyr::ungroup() %>%
dplyr::group_by(id) %>%
dplyr::filter(wr != bcsnowdata::wtr_yr(Sys.Date())) %>%
unique() %>%
dplyr::mutate(numberofyears = length(percent_available)) %>%
dplyr::select(id, numberofyears) %>%
unique()
# Bind stats regarding the number of years available together
df_range <- dplyr::full_join(daterange, numberyears, by = c("id"))
# Bind to the statistics
df_stat_date <- dplyr::full_join(df_stat, df_range, by = c("id"))
# get the day of the max and min!!
min_date <- df_hist %>%
dplyr::ungroup() %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.min(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_min_utc = date_utc)
# Get the date of the max value for the date
max_date <- df_hist %>%
dplyr::ungroup() %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.max(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_max_utc = date_utc)
# append to data
dates <- dplyr::full_join(min_date, max_date, by = c("id", "m_d"))
df_stat_1 <- dplyr::full_join(df_stat_date, dates, by = c("id", "m_d"))
data = df_time
View(data)
# if the user input data as a station name (i.e., the function is being used as a stand alone function), get the data for the station
if (all(data %in% bcsnowdata::snow_auto_location()$LOCATION_ID)) {
data_norm <- bcsnowdata::get_aswe_databc(
station_id = data,
get_year = "All",
parameter = "swe",
timestep = "daily") %>%
dplyr::rename("values_stats" = value)
} else if (all(data %in% bcsnowdata::snow_manual_location()$LOCATION_ID)) {
data_norm <- bcsnowdata::get_manual_swe(
station_id = data,
get_year = "All",
survey_period = "All")
} else {
data_norm <- data
}
data_norm
data = df_time
all(data %in% bcsnowdata::snow_auto_location()$LOCATION_ID)
# if the user input data as a station name (i.e., the function is being used as a stand alone function), get the data for the station
if (all(data %in% bcsnowdata::snow_auto_location()$LOCATION_ID)) {
data_norm <- bcsnowdata::get_aswe_databc(
station_id = data,
get_year = "All",
parameter = "swe",
timestep = "daily") %>%
dplyr::rename("values_stats" = value)
} else if (all(data %in% bcsnowdata::snow_manual_location()$LOCATION_ID)) {
data_norm <- bcsnowdata::get_manual_swe(
station_id = data,
get_year = "All",
survey_period = "All")
} else {
data_norm <- data
}
all(data %in% bcsnowdata::snow_auto_location()$LOCATION_ID)
all(data %in% bcsnowdata::snow_manual_location()$LOCATION_ID)
data
# if the user input data as a station name (i.e., the function is being used as a stand alone function), get the data for the station
if (all(data %in% bcsnowdata::snow_auto_location()$LOCATION_ID)) {
data_norm <- bcsnowdata::get_aswe_databc(
station_id = data,
get_year = "All",
parameter = "swe",
timestep = "daily") %>%
dplyr::rename("values_stats" = value)
} else if (all(data %in% bcsnowdata::snow_manual_location()$LOCATION_ID)) {
data_norm <- bcsnowdata::get_manual_swe(
station_id = data,
get_year = "All",
survey_period = "All")
} else {
data_norm <- data
}
id <- unique(data_norm$id)
if (dim(data_norm)[1] == 0) {
df_normals_out <- data.frame(station_id = character())
} else if (any(id %in% bcsnowdata::snow_auto_location()$LOCATION_ID)) { # Check to see whether the station is a manual or automated station
data_id <- "value"
aswe <- bcsnowdata::snow_auto_location()$LOCATION_ID
# filter data for ASWE sites
data_swe <- data_norm %>%
dplyr::filter(id %in% aswe)
# Use the aswe_normal() function to fill in data (if appropriate) and calculate normals (if there is sufficient data)
df_normals_aswe <- aswe_normal(data = data_swe, normal_max, normal_min, data_id = "values_stats", force = force)
# If the site is manual site
} else if (any(id %in% bcsnowdata::snow_manual_location()$LOCATION_ID)) {
data_id <- "swe_mm"
# filter data for ASWE sites
data_man <- data_norm %>%
dplyr::filter(id %in% bcsnowdata::snow_manual_location()$LOCATION_ID)
df_normals_man <- manual_normal_prep(data = data_man, normal_max = normal_max, normal_min = normal_min, data_id = data_id)
} else if (id %in% snow_basins()) {
# if you are trying to simply get the normal for the entire basin, take the average across the data
df_normals_basin <- basin_normal(data = data_norm, normal_max = normal_max, normal_min = normal_min)
}
exists("df_normals_aswe") && exists("df_normals_man")
exists("df_normals_man")
any(id %in% bcsnowdata::snow_manual_location()$LOCATION_ID)
id <- unique(data_norm$id)
id
dim(data_norm)[1] == 0
any(id %in% bcsnowdata::snow_auto_location()$LOCATION_ID)
any(id %in% bcsnowdata::snow_manual_location()$LOCATION_ID)
if (dim(data_norm)[1] == 0) {
df_normals_out <- data.frame(station_id = character())
} else if (any(id %in% bcsnowdata::snow_auto_location()$LOCATION_ID)) { # Check to see whether the station is a manual or automated station
data_id <- "value"
aswe <- bcsnowdata::snow_auto_location()$LOCATION_ID
# filter data for ASWE sites
data_swe <- data_norm %>%
dplyr::filter(id %in% aswe)
# Use the aswe_normal() function to fill in data (if appropriate) and calculate normals (if there is sufficient data)
df_normals_aswe <- aswe_normal(data = data_swe, normal_max, normal_min, data_id = "values_stats", force = force)
# If the site is manual site
} else if (any(id %in% bcsnowdata::snow_manual_location()$LOCATION_ID)) {
data_id <- "swe_mm"
# filter data for ASWE sites
data_man <- data_norm %>%
dplyr::filter(id %in% bcsnowdata::snow_manual_location()$LOCATION_ID)
df_normals_man <- manual_normal_prep(data = data_man, normal_max = normal_max, normal_min = normal_min, data_id = data_id)
} else if (id %in% snow_basins()) {
# if you are trying to simply get the normal for the entire basin, take the average across the data
df_normals_basin <- basin_normal(data = data_norm, normal_max = normal_max, normal_min = normal_min)
}
remove(df_normals_man)
exists("df_normals_aswe") && exists("df_normals_man")
exists("df_normals_aswe") && !(exists("df_normals_man"))
!(exists("df_normals_aswe")) && exists("df_normals_man")
!(exists("df_normals_aswe")) && !(exists("df_normals_man"))
if (exists("df_normals_aswe") && exists("df_normals_man") ) {
df_normals_out <- list(df_normals_aswe, df_normals_man)
}
if (exists("df_normals_aswe") && !(exists("df_normals_man"))) {
df_normals_out <- df_normals_aswe
}
if (!(exists("df_normals_aswe")) && exists("df_normals_man")) {
df_normals_out <- df_normals_man
}
if (!(exists("df_normals_aswe")) && !(exists("df_normals_man")) && exists("df_normals_basin")) {
df_normals_out <- df_normals_basin
}
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "README-"
)
# Return statistics for 2F18P for March 1, 2021. Use a normal period from 1991-2020. Force the recalculation of the normals.
library(bcsnowstats)
library(bcsnowdata)
stats_ASWE_west <- get_snow_stats(station_id = "2C09Q",
survey_period = "03-01",
get_year = "2021",
normal_min = 1991,
normal_max = 2020,
force = TRUE)
stats_ASWE_west <- get_snow_stats(station_id = "2C09Q",
survey_period = "03-01",
get_year = "2021",
normal_min = 1991,
normal_max = 2020,
force = TRUE)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "README-"
)
# Return statistics for 2F18P for March 1, 2021. Use a normal period from 1991-2020. Force the recalculation of the normals.
library(bcsnowstats)
library(bcsnowdata)
stats_ASWE_west <- get_snow_stats(station_id = "2C09Q",
survey_period = "03-01",
get_year = "2021",
normal_min = 1991,
normal_max = 2020,
force = TRUE)
station_id = "2C09Q"
survey_period = "03-01"
get_year = "2021"
normal_min = 1991
normal_max = 2020
force = TRUE
# Check to see whether the station is a manual or automated station
id_aswe <- station_id[station_id %in% bcsnowdata::snow_auto_location()$LOCATION_ID]
id_manual <- station_id[station_id %in% bcsnowdata::snow_manual_location()$LOCATION_ID]
if (length(id_aswe) > 0) {
df_aswe <- stats_aswe(station_id = id_aswe,
survey_period = survey_period,
get_year = get_year,
normal_min = normal_min,
normal_max = normal_max,
force = force)
}
library(bcsnowstats)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "README-"
)
# Return statistics for 2F18P for March 1, 2021. Use a normal period from 1991-2020. Force the recalculation of the normals.
library(bcsnowstats)
library(bcsnowdata)
stats_ASWE_west <- get_snow_stats(station_id = "2C09Q",
survey_period = "03-01",
get_year = "2021",
normal_min = 1991,
normal_max = 2020,
force = TRUE)
station_id = "2C09Q"
survey_period = "03-01"
get_year = "2021"
normal_min = 1991
normal_max = 2020
force = TRUE
# Check to see whether the station is a manual or automated station
id_aswe <- station_id[station_id %in% bcsnowdata::snow_auto_location()$LOCATION_ID]
id_manual <- station_id[station_id %in% bcsnowdata::snow_manual_location()$LOCATION_ID]
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
tims_start <- Sys.time()
test <- get_snow_stats(station_id = "2C09Q",
survey_period = "All",
get_year = "2022",
normal_min = 1991,
normal_max = 2020,
force = TRUE)
time <- tims_start - Sys.time()
View(test)
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "README-"
)
# Return statistics for 2F18P for March 1, 2021. Use a normal period from 1991-2020. Force the recalculation of the normals.
library(bcsnowstats)
library(bcsnowdata)
stats_ASWE_west <- get_snow_stats(station_id = "2C09Q",
survey_period = "03-01",
get_year = "2021",
normal_min = 1991,
normal_max = 2020,
force = TRUE)
head(stats_ASWE_west)
# Get statistics for 5 manual snow stations for March 1st, 2021. Use a normal period of 1991-2020.
# Calculate normals from 1991-2020 for an automated station 2C09Q - Morrisey Ridge. Force the re-download of data.
normal_aswe <- SWE_normals(data = "2C09Q", normal_max = 2020, normal_min = 1991, force = TRUE)
# Calculate normals from 1991-2020 for an manual station 4E01. Force the re-download of data.
normal_manual <- SWE_normals(data = "4E01", normal_max = 2020, normal_min = 1991, force = TRUE)
plot_test <- plot_interactive_aswe(id = "1D08P",
save = FALSE)
plot_test
View(plot_test)
plot_test$SWEplot
plot_test <- plot_interactive_aswe(id = "1A01P",
save = FALSE)
plot_test$SWEplot
