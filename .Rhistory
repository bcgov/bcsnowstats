dplyr::select(Station_ID, ELEVATION, LOCATION_NAME, geometry) %>%
dplyr::rename(Elevation = ELEVATION, Name = LOCATION_NAME)
stations_el <- bind_rows(auto, manual_data)
}
# =====================
#' Function for assigning a basin name to a station ID
#' @param id id of station you are assigning to a basin
#' @param basin name of basin
#' @param exceptions stations to skip
#' @importFrom data.table %like%
#' @export
#' @keywords snow site basin name
#' @examples \dontrun{}
basin_name <- function(id = "All", basin = "All", exceptions = NULL) {
# get all of the sites within the archive
all_sites <- unique(c(bcsnowdata::snow_auto_location()$LOCATION_ID, bcsnowdata::snow_manual_location()$LOCATION_ID))
# Apply exceptions - remove sites that should be removed
all_sites <-  all_sites[!all_sites %in% exceptions]
# associate basins by the ID number
basins_all <- snow_basins()
sites_first <- data.frame(Basin = basins_all, Station_ID_used = 2)
sites_first[sites_first$Basin == "UpperFraserWest",][2] <- paste(c("1A04", "1A07", "1A12", "1A16", "1A23", "1A24", "1A12P"), collapse = ";")
sites_first[sites_first$Basin == "UpperFraserEast",][2] <- paste(c("1A01P", "1A02P", "1A03P", "1A05P", "1A14P", "1A15P", "1A17P", "1A19P",
"1A05","1A06", "1A06A","1A08", "1A09", "1A10",
"1A11", "1A13", "1A15", "1A18", "1A20", "1A21", "1A22"), collapse = ";")
sites_first[sites_first$Basin == "Nechako",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "B"& substring(all_sites, 1, 1) == "1"), collapse = ";")
sites_first[sites_first$Basin == "MiddleFraser",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "C" & substring(all_sites, 1, 1) == "1"), collapse = ";")
sites_first[sites_first$Basin == "LowerFraser",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "D" & substring(all_sites, 1, 1) == "1"), collapse = ";")
sites_first[sites_first$Basin == "NorthThompson",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "E" & substring(all_sites, 1, 1) == "1"), collapse = ";")
sites_first[sites_first$Basin == "SouthThompson",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "F" & substring(all_sites, 1, 1) == "1"), collapse = ";")
sites_first[sites_first$Basin == "UpperColumbia",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "A" & substring(all_sites, 1, 1) == "2"), collapse = ";")
sites_first[sites_first$Basin == "WestKootenay",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) %in% c("D","B") & substring(all_sites, 1, 1) == "2"), collapse = ";")
sites_first[sites_first$Basin == "EastKootenay",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "C" & substring(all_sites, 1, 1) == "2"), collapse = ";")
sites_first[sites_first$Basin == "Okanagan",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "F" & substring(all_sites, 1, 1) == "2"), collapse = ";")
sites_first[sites_first$Basin == "Boundary",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "E" & substring(all_sites, 1, 1) == "2"), collapse = ";")
sites_first[sites_first$Basin == "Similkameen",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "G" & substring(all_sites, 1, 1) == "2"), collapse = ";")
sites_first[sites_first$Basin == "SouthCoast",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "A" & substring(all_sites, 1, 1) == "3"), collapse = ";")
sites_first[sites_first$Basin == "VancouverIsland",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "B" & substring(all_sites, 1, 1) == "3"), collapse = ";")
sites_first[sites_first$Basin == "CentralCoast",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "C" & substring(all_sites, 1, 1) == "3"), collapse = ";")
sites_first[sites_first$Basin == "Skagit",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "D" & substring(all_sites, 1, 1) == "3"), collapse = ";")
sites_first[sites_first$Basin == "Peace",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "A" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "SkeenaNass",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "B" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "Liard",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "C" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "Stikine",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "D" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "Northwest",][2] <- paste(subset(all_sites, substring(all_sites, 2, 2) == "E" & substring(all_sites, 1, 1) == "4"), collapse = ";")
sites_first[sites_first$Basin == "HaidaGwaii",][2] <- paste(NA, collapse = ";")
# basin not on the map currenly - future
sites_first[sites_first$Basin == "Nicola_old",][2] <- paste(c("1C01",	"1C09",	"1C19",	"1C25",	"1C29",	"2F13",	"2F18",	"2F23",	"2F24"), collapse = ";")
sites_first[sites_first$Basin == "FraserPlateau",][2] <- paste(c("1C08", "1C22", "1C21"), collapse = ";")
sites_first[sites_first$Basin == "LillBridge",][2] <- paste(c("1C06", "1C39", "1C38P", "1C38", "1C40P", "1C40", "1C12P", "1C14P", "1C14", "1C37", "1C05P", "1C05", "1C18P", "1C28"), collapse = ";")
sites_first[sites_first$Basin == "Quesnel",][2] <- paste(c("1C33A", "1C13A", "1C17", "1C20P", "1C23", "1C41P"), collapse = ";")
sites_first[sites_first$Basin == "LowerThompson",][2] <- paste(c("1C32", "1C09A", "1C19", "1C25", "1C29", "1C29P", "1C01"), collapse = ";")
sites_first[sites_first$Basin == "Fraser",][2] <- paste(subset(all_sites, substring(all_sites, 1, 1) == "1"), collapse = ";")
sites_first[sites_first$Basin == "Province",][2] <- paste(all_sites, collapse = ";")
if (id == "All") {
sites_id <- sites_first
} else {
# find the basin within the dataframe and return the basin name
sites_id <- sites_first[sites_first$Station_ID_used %like% id, ] %>%
dplyr::select(Basin) %>%
dplyr::distinct(Basin)
}
# Subset by the basin you want
if (basin == "All"){
sites_final <- sites_id
} else {
sites_final <- sites_id %>%
dplyr::filter(Basin %in% basin)
}
return(sites_final)
}
# ===============
#' Return the name of all snow basins within BC
#' @export
#' @keywords snow basin names
#' @examples \dontrun{}
snow_basins <- function() {
return(c("UpperFraserWest", "UpperFraserEast", "Nechako", "MiddleFraser", "LowerFraser", "NorthThompson",
"SouthThompson", "UpperColumbia", "WestKootenay", "EastKootenay", "Okanagan", "Boundary", "Similkameen", "SouthCoast",
"VancouverIsland", "CentralCoast", "Skagit", "Peace", "SkeenaNass", "Stikine", "Liard", "Northwest", "HaidaGwaii",
"Nicola_old", "FraserPlateau", "LillBridge", "Quesnel", "LowerThompson", "Fraser", "Province"))
}
# ======================
#' Legal annotation for plots
#' @export
#' @keywords internal
#' @examples \dontrun{}
annotation <- function() {
paste0("<b>Users should use the information on this website with caution and at their own risk.</b>", "<br>",
"Reproduction and analysis of data published by the BC Ministry of Environment, including data collected by affilated partners (more information through <a href= 'https://www2.gov.bc.ca/gov/content/environment/air-land-water/water/water-science-data/water-data-tools/snow-survey-data'>Snow Survey Data)</a>",
"<br>", " has not been produced in affiliation with or with the endorsement of the Ministry of Environment.")
}
# =================
#' Colour palette
#' @export
#' @keywords internal
#' @examples \dontrun{}
colour_p <- function() {
# Colours from Sam Albers's map - March 2019
colour_hex <- c("#FFFFFF", "#E50000", "#E69800", "#FFD380", "#FFFF00",
"#AAFF01", "#00FEC4", "#01C5FF", "#0071FE", "#002573")
colours_v <- viridis::viridis(1000)
# normal
colour_norm <- "#482475FF"
# curent year
colour_curr <- "#43BE71FF"
# mean
colour_mean <- "#7AD151FF"
colour_palette <- list(colour_hex = colour_hex, colours_v = colours_v, colour_norm = colour_norm, colour_curr = colour_curr, colour_mean = colour_mean)
}
# Fill in NA values with 0 is there is no snow at the pillow
if (dim(df_tmp_1)[1] > 0) {
df_stat_fill <- fillNA0(data = df_tmp_1)
} else {
df_stat_fill <- df_tmp_1
}
# ===========
# Calculate statistics through function. return a table of statistics for each day of the year
# ===========
# Compile this as a cache to speed up the function?
df_stat <- snow_stats(data = df_stat_fill, data_id = "value", normal_min, normal_max, force)
df_stat_fill
data = df_stat_fill
data_id = "value"
# Ensure that current water year is defined
current_wy <- bcsnowdata::wtr_yr(Sys.Date())
# compute historical stats - for each day of the year from historical stats (prior to this water year)
# get historic dataset - previous to this year
df_hist <- data %>%
dplyr::filter(wr < current_wy) %>% # Filter out the current year data prior to calculating statistics
dplyr::group_by(id, m_d) %>% # Group by the station ID and the month/day
dplyr::rename(values_stats = all_of(data_id)) %>% # The user can define what data to run the statistics on. Usually this is the daily mean
dplyr::filter(!is.na(values_stats)) # filter out missing data
# Calculate the statistics on the data you specify
df_stat <- do.call(data.frame,
list(dplyr::summarise(df_hist, min = min(values_stats, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, swe_mean = mean(values_stats, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q5 = quantile(values_stats, 0.05, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q10 = quantile(values_stats, 0.1, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q25 = quantile(values_stats, 0.25, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q50 = quantile(values_stats, 0.5,na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q75 = quantile(values_stats,0.75, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, Q90 = quantile(values_stats,0.90, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(df_hist, max = max(values_stats, na.rm = TRUE), .groups = "keep"))) %>%
dplyr::select(-m_d.1, -m_d.2, -m_d.3, -m_d.4, -m_d.5, -m_d.6, -m_d.7, -m_d.8) %>%
dplyr::select(-id.1, -id.2, -id.3, -id.4, -id.5, -id.6, -id.7, -id.8)
# -------------------------------
# Get the max, min dates, as well as the data range and number of years
# Number of years. Implement threshold
df_time <- data %>%
dplyr::rename(values_stats = all_of(data_id)) %>% # The user can define what data to run the statistics on. Usually this is the daily mean
dplyr::ungroup() %>%
dplyr::filter(!is.na(values_stats)) # filter out NA values to get more accurate start and end dates
# Get the years that have >80% of data coverage during the snow accumulation period! Oct 1 - June 30
df_time_2 <- df_time %>%
dplyr::arrange(id, date_utc) %>%
dplyr::ungroup() %>%
dplyr::group_by(id, wr) %>%
dplyr::filter(m_d >= "10-01" | m_d <= "06-30") %>%
dplyr::mutate(percent_available = length(values_stats) / as.numeric(abs(difftime(as.POSIXct("2018-10-01"), as.POSIXct("2019-06-30"), units = "days")))*100) %>%
dplyr::select(id, wr, percent_available) %>%
unique() %>%
dplyr::filter(wr == bcsnowdata::wtr_yr(Sys.Date()) | percent_available >= 50)
daterange <- df_time_2 %>%
dplyr::ungroup() %>%
dplyr::group_by(id) %>%
dplyr::summarize(maxdate = max(wr), mindate = min(wr), .groups = "keep") %>%
dplyr::mutate(data_range = (paste0(mindate, " to ", maxdate)))
numberyears <- df_time_2 %>%
dplyr::ungroup() %>%
dplyr::group_by(id) %>%
dplyr::filter(wr != bcsnowdata::wtr_yr(Sys.Date())) %>%
unique() %>%
dplyr::mutate(numberofyears = length(percent_available)) %>%
dplyr::select(id, numberofyears) %>%
unique()
# Bind stats regarding the number of years available together
df_range <- dplyr::full_join(daterange, numberyears, by = c("id"))
# Bind to the statistics
df_stat_date <- dplyr::full_join(df_stat, df_range, by = c("id"))
# get the day of the max and min!!
min_date <- df_hist %>%
dplyr::ungroup() %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.min(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_min_utc = date_utc)
# Get the date of the max value for the date
max_date <- df_hist %>%
dplyr::ungroup() %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.max(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_max_utc = date_utc)
# append to data
dates <- dplyr::full_join(min_date, max_date, by = c("id", "m_d"))
df_stat_1 <- dplyr::full_join(df_stat_date, dates, by = c("id", "m_d"))
### Calculate normals using function.
# Function contains all of the thresholds, etc that make it
df_normals_1 <- SWE_normals(data = df_time, normal_max, normal_min, force)
data = df_time
force
# if the user input data as a station name (i.e., the function is being used as a stand alone function), get the data for the station
if (all(data %in% bcsnowdata::snow_auto_location()$LOCATION_ID)) {
data_norm <- bcsnowdata::get_aswe_databc(
station_id = data,
get_year = "All",
parameter = "swe",
timestep = "daily") %>%
dplyr::rename("values_stats" = value)
} else if (all(data %in% bcsnowdata::snow_manual_location()$LOCATION_ID)) {
data_norm <- bcsnowdata::get_manual_swe(
station_id = data,
get_year = "All",
survey_period = "All")
} else {
data_norm <- data
}
id <- unique(data_norm$id)
if (dim(data_norm)[1] == 0) {
df_normals_out <- data.frame(station_id = character())
} else if (any(id %in% bcsnowdata::snow_auto_location()$LOCATION_ID)) { # Check to see whether the station is a manual or automated station
data_id <- "value"
aswe <- bcsnowdata::snow_auto_location()$LOCATION_ID
# filter data for ASWE sites
data_swe <- data_norm %>%
dplyr::filter(id %in% aswe)
# Use the aswe_normal() function to fill in data (if appropriate) and calculate normals (if there is sufficient data)
df_normals_aswe <- aswe_normal(data = data_swe, normal_max, normal_min, data_id = "values_stats", force = force)
# If the site is manual site
} else if (any(id %in% bcsnowdata::snow_manual_location()$LOCATION_ID)) {
data_id <- "swe_mm"
# filter data for ASWE sites
data_man <- data_norm %>%
dplyr::filter(id %in% bcsnowdata::snow_manual_location()$LOCATION_ID)
df_normals_man <- manual_normal_prep(data = data_man, normal_max = normal_max, normal_min = normal_min, data_id = data_id)
} else if (id %in% snow_basins()) {
# if you are trying to simply get the normal for the entire basin, take the average across the data
df_normals_basin <- basin_normal(data = data_norm, normal_max = normal_max, normal_min = normal_min)
}
dim(data_norm)[1] == 0
any(id %in% bcsnowdata::snow_auto_location()$LOCATION_ID)
data_id <- "value"
aswe <- bcsnowdata::snow_auto_location()$LOCATION_ID
# filter data for ASWE sites
data_swe <- data_norm %>%
dplyr::filter(id %in% aswe)
# Use the aswe_normal() function to fill in data (if appropriate) and calculate normals (if there is sufficient data)
df_normals_aswe <- aswe_normal(data = data_swe, normal_max, normal_min, data_id = "values_stats", force = force)
data = data_swe
data_id = "values_stats"
# Check to ensure that the ASWE archived data has been cached on the user's computer and is up to date
fname <- paste0(unique(data$parameter), "_norm_archive.rds")
dir <- data_dir()
data_dir <- function() {
if (R.Version()$major >= 4) {
getOption("bcsnowstats.data_dir", default = tools::R_user_dir("bcsnowstats", "cache"))
} else {
getOption("bcsnowstats.data_dir", default = rappdirs::user_cache_dir("bcsnowstats"))
}
}
show_cached_files <- function() {
file.path(list.files(data_dir(), full.names = TRUE))
}
check_write_to_data_dir <- function(dir, ask) {
if (ask) {
ans <- gtools::ask(paste("bcsnowstats would like to store this layer in the directory:",
dir, "Is that okay?", sep = "\n"))
if (!(ans %in% c("Yes", "YES", "yes", "y"))) stop("Exiting...", call. = FALSE)
}
if (!dir.exists(dir)) {
message("Creating directory to hold bcsnowstats data at ", dir)
dir.create(dir, showWarnings = FALSE, recursive = TRUE)
} else {
message("Saving to bcsnowstats data directory at ", dir)
}
}
# Check to ensure that the ASWE archived data has been cached on the user's computer and is up to date
fname <- paste0(unique(data$parameter), "_norm_archive.rds")
dir <- data_dir()
fpath <- file.path(dir, fname)
any(!file.exists(fpath)) | force
# Check that the directory exists
check_write_to_data_dir(dir, ask)
ask = FALSE
# Check that the directory exists
check_write_to_data_dir(dir, ask)
# Calculate the normal data for all days of the year
df_normals_out  <- int_aswenorm(data, normal_max, normal_min, data_id)
data
# Put data into right format using the data_massage function
data_m <- data_massage(data)
if ("swe_mean" %in% colnames(data_m)) {
data_id <- "swe_mean" # reassign the data_ID value
}
# Filter the data by the normal span that you specify
df_normal_time <- data_m %>%
dplyr::filter(wr <= normal_max, wr >= normal_min) %>% # Filter by the normal dates that you specify
dplyr::group_by(id, m_d) %>%
dplyr::rename(values_stats = all_of(data_id))
# ++++++++++++++++++++++ thresholds
# Check to see whether there is sufficient data to calculate a normal.
# The WMO recommends only calculating a normal for stations that have 80% of the data available
# Firstly, just show the amount of data available for the normal period
# Number of years with 80% or great of the data available.
# Only count the data between Oct-June - doesn't matter if the snow data is missing in summer - 273 days in snow accumulation/melt season
# Only for ASWE
df_normal_80 <- df_normal_time %>%
dplyr::filter(!is.na(values_stats)) %>% # # filter out missing data
dplyr::ungroup() %>%
dplyr::group_by(wr) %>%
dplyr::filter(lubridate::month(as.Date(m_d, format = "%m-%d")) <= 6 || lubridate::month(as.Date(m_d, format = "%m-%d")) >= 10) %>% # get only the snow accumulation and melt season
dplyr::mutate(percent_available = length(values_stats) / length(seq(as.Date("2020-10-01"), as.Date("2021-06-30"), by = "day")) * 100) %>%
dplyr::select(wr, percent_available) %>%
unique() %>%
dplyr::filter(percent_available >= 80) # filter by the 80% WMO threshold
# Get the number of years within the normal range with >= 80% data coverage within a specific year
numberofyears_80 <- dim(df_normal_80)[1]
# Add the number of years with 80% of data to the dataframe
df_normal_time$numberofyears_80_raw <- dim(df_normal_80)[1]
# =============================
# Fill in data depending on how many years of data there are available
# Is there less than 10 years of data?
if (numberofyears_80 < 10) {
data_0t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# ++++++++++++++++++++++++++++++++++++++++++++++
# Use function to check to see if there is manual site to extend data.
# For now, do not calculate a normal
# Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columms
all_swe <- data_0t10 %>%
dplyr::filter(wr %in% df_normal_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# Does the station have between 10-20 years of data? If so, extend the dataset using 1) manual dataset (if converted), and 2) adjacent stations
if (numberofyears_80 >= 10 && numberofyears_80 < 20) {
data_20t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# Fill in missing data with an estimated dataset from either manual dataset (if converted) and/or adjacent stations
all_swe <- snow_datafill(data_soi = data_20t10, data_id, normal_max, normal_min)
}
# Does the site have between 20-30 years of data? Don't add in any additional data and jsut calculcate normals from
if (numberofyears_80 >= 20 && numberofyears_80 <= 30) {
# DON'T Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columns
all_swe <- df_normal_time %>%
#dplyr::filter(wr %in% df_normal_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
numberofyears_80 >= 10 && numberofyears_80 < 20
data_20t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
data_soi = data_20t10
unique(data_soi$id) %in% bcsnowdata::snow_auto_location()$LOCATION_ID
# Was the site converted from a manual site to a ASWE site? Get the name without the P (P shows that the site is ASWE)
station_id_manual <- substring(unique(data_soi$id), 1, 4)
# ==================
# Fill in any missing data with data interpolated from stations within 100km of the station using a normal ratio method
# Are there stations within 100 km?
# First, get the location of the station you are looking at
location_station <- bcsnowdata::snow_auto_location() %>%
dplyr::filter(LOCATION_ID %in% unique(data_soi$id))
location_station <- sf::st_as_sf(location_station)
# All other sites within the vicinity
location_all <- bcsnowdata::snow_auto_location()
location_all <- sf::st_as_sf(location_all) %>%
dplyr::filter(!(LOCATION_ID %in% unique(data_soi$id)))
# 100 km buffer around the site
fr_buffer <- sf::st_buffer(location_station, dist = 1e5)
# Filter for those sites within a 100 km buffer
ASWE_100km <- sf::st_filter(location_all, fr_buffer)
length(unique(ASWE_100km$LOCATION_ID)) > 0
# If there are ASWE sites within a 100 km radius, the, proceed with the normal ratio method to backfill missing data
if (length(unique(ASWE_100km$LOCATION_ID)) > 0) {
############## Get the weight of each station
# Get the data for the first station
stations_adj <- unique(ASWE_100km$LOCATION_ID)
# use function to return data estimated from normal ratio method using stations within 100 km of the station of interest
all_swe <- aswe_normalratio(data_soi, stations_adj, data_id, normal_max, normal_min)
} else {
# If there are no stations within a 100 km radius and 20-10 years of data, then do not calculate a normal.
all_swe <- data_soi %>%
dplyr::arrange(date_utc) %>%
dplyr::mutate(swe_fornormal = values_stats)  # make a new column that clearly shows the data to use for normal calculation. If there is 20-10 years of data, and no nearby stations to estimate data, no normal calculated
}
length(unique(ASWE_100km$LOCATION_ID)) > 0
############## Get the weight of each station
# Get the data for the first station
stations_adj <- unique(ASWE_100km$LOCATION_ID)
# Format the data from the station of interest  - interpolate for missing data using linear interpolation
data_soi <- data_soi %>%
dplyr::ungroup() %>%
dplyr::mutate(swe_interp = zoo::na.approx(values_stats, na.rm = F, maxgap = 21)) %>%
dplyr::group_by(m_d)
# Take the daily mean
data_soi_m <- do.call(data.frame,
list(dplyr::summarise(data_soi, mean_swe_day = mean(values_stats, na.rm = TRUE), .groups = "keep"))) %>%
dplyr::mutate(date = as.Date(paste0("2020-", m_d))) %>%
dplyr::mutate(mean_swe_day_7 = zoo::rollmean(mean_swe_day, k = 7, na.pad = TRUE, align = c("center"))) %>% # Apply 7 day smoothing to data
dplyr::mutate(mean_swe_day_7 = ifelse(is.na(mean_swe_day_7), mean_swe_day, mean_swe_day_7)) %>% # Get rid of leading NA values
dplyr::mutate(id = unique(data_soi$id))
# Fill any missing data using interpolation
data_soi_m$mean_swe_day_7_fill <- zoo::na.approx(data_soi_m$mean_swe_day_7, na.rm = T, maxgap = 7) # Fill any gaps with linear interpolation
# ------------------------------------------------------
# Calculate the estimated SWE using the ratio normal method (for mean SWE for each day)
ratio_all <- lapply(stations_adj[5], ratio_function,
data_station_oi = data_soi_m,
data_id, normal_max, normal_min)
# Format the data from the station of interest  - interpolate for missing data using linear interpolation
data_soi <- data_soi %>%
dplyr::ungroup() %>%
dplyr::mutate(swe_interp = zoo::na.approx(values_stats, na.rm = F, maxgap = 21)) %>%
dplyr::group_by(m_d)
# Take the daily mean
data_soi_m <- do.call(data.frame,
list(dplyr::summarise(data_soi, mean_swe_day = mean(values_stats, na.rm = TRUE), .groups = "keep"))) %>%
dplyr::mutate(date = as.Date(paste0("2020-", m_d))) %>%
dplyr::mutate(mean_swe_day_7 = zoo::rollmean(mean_swe_day, k = 7, na.pad = TRUE, align = c("center"))) %>% # Apply 7 day smoothing to data
dplyr::mutate(mean_swe_day_7 = ifelse(is.na(mean_swe_day_7), mean_swe_day, mean_swe_day_7)) %>% # Get rid of leading NA values
dplyr::mutate(id = unique(data_soi$id))
# Fill any missing data using interpolation
data_soi_m$mean_swe_day_7_fill <- zoo::na.approx(data_soi_m$mean_swe_day_7, na.rm = T, maxgap = 7) # Fill any gaps with linear interpolation
stations_adj[5]
stations_adj
aswe_normalratio <- function(data_soi, stations_adj, data_id, normal_max, normal_min) {
# Format the data from the station of interest  - interpolate for missing data using linear interpolation
data_soi <- data_soi %>%
dplyr::ungroup() %>%
dplyr::mutate(swe_interp = zoo::na.approx(values_stats, na.rm = F, maxgap = 21)) %>%
dplyr::group_by(m_d)
# Take the daily mean
data_soi_m <- do.call(data.frame,
list(dplyr::summarise(data_soi, mean_swe_day = mean(values_stats, na.rm = TRUE), .groups = "keep"))) %>%
dplyr::mutate(date = as.Date(paste0("2020-", m_d))) %>%
dplyr::mutate(mean_swe_day_7 = zoo::rollmean(mean_swe_day, k = 7, na.pad = TRUE, align = c("center"))) %>% # Apply 7 day smoothing to data
dplyr::mutate(mean_swe_day_7 = ifelse(is.na(mean_swe_day_7), mean_swe_day, mean_swe_day_7)) %>% # Get rid of leading NA values
dplyr::mutate(id = unique(data_soi$id))
# Fill any missing data using interpolation
data_soi_m$mean_swe_day_7_fill <- zoo::na.approx(data_soi_m$mean_swe_day_7, na.rm = T, maxgap = 7) # Fill any gaps with linear interpolation
# ------------------------------------------------------
# Calculate the estimated SWE using the ratio normal method (for mean SWE for each day)
ratio_all <- lapply(stations_adj, ratio_function,
data_station_oi = data_soi_m,
data_id, normal_max, normal_min)
ratio_all_unfold <- do.call(rbind, ratio_all)
# Unfold the data
if (dim(ratio_all_unfold)[1] > 0 & !(all(is.na(ratio_all_unfold)))) { # Calculated the estimated SWE for the entire dataset available (if there is data available!)
# Unfold the estimated data from the adjacent stations
estimated_unmelted <- ratio_all_unfold %>%
dplyr::filter(!is.na(estimated_swe)) %>%
tidyr::spread(id, estimated_swe)
if ("NA" %in% colnames(estimated_unmelted)) {
estimated_unmelted <- estimated_unmelted %>%
dplyr::select(-"NA")
}
# If there are multiple stations, take the row mean. If not, use the one station retrieved
estimated_unmelted_mean <- estimated_unmelted %>%
dplyr::mutate(mean_est_swe = rowMeans(dplyr::select(., -date_utc), na.rm = TRUE)) %>%
dplyr::mutate(date_utc = as.Date(date_utc)) #%>%
#dplyr::mutate(est_swe_5 = zoo::rollmean(mean_est_swe, k = 7, na.pad = TRUE, align = c("center")))
# Join the two datasets together and create a column that is the observed data with the estimated filled in
all_swe_ratio <- dplyr::full_join(data_soi, estimated_unmelted_mean %>% dplyr::select(date_utc, mean_est_swe), by = "date_utc") %>%
dplyr::arrange(date_utc) %>%
dplyr::mutate(swe_est = ifelse(is.na(swe_interp), mean_est_swe, swe_interp))  #if there is no interpolated swe value, fill
all_swe_ratio$swe_est_7 <- zoo::rollmean(as.numeric(all_swe_ratio$swe_est), k = 14, na.pad = TRUE, align = c("center")) # not working in pipe?
all_swe_ratio <- all_swe_ratio %>%
dplyr::mutate(swe_fornormal = swe_est_7) %>% # make a new column that clearly shows the data to use for normal calculation
dplyr::mutate(id = unique(data_soi$id, na.rm = TRUE)) %>%
dplyr::mutate(wr = bcsnowdata::wtr_yr(date_utc)) %>%
dplyr::mutate(m_d = format.Date(date_utc, "%m-%d"))
# Plot the estimated dataset versus the observed
#ggplot() +
#   geom_point(data = all_swe_ratio, aes(x = date_utc, y = swe_fornormal), colour = "blue") +
# geom_point(data = all_swe_ratio, aes(x = date_utc, y = values_stats))
} else {
# If there are no stations within a 100 km radius and 20-10 years of data THAT HAVE DATA, then do not calculate a normal.
all_swe_ratio <- data_soi %>%
dplyr::arrange(date_utc) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# Put the above into a function?
# Calculate the standard error for the normal ratio method
rmse_ratio_data <- all_swe_ratio %>%
dplyr::mutate(residual = values_stats - swe_fornormal) %>%
dplyr::filter(!is.na(residual)) %>%
dplyr::mutate(residual_2 = residual^2)
all_swe_ratio$rmse_ratio <- sqrt(1 / length(rmse_ratio_data$residual) * sum(rmse_ratio_data$residual_2))
# -----------------------------------------------
# Calculate the estimated SWE using a multiple linear regression
#all_swe_linear <- aswe_linearmodel(data_soi_m, stations_adj)
# Choose which data to return based on the lowest RMSE
return(all_swe_ratio)
}
devtools::document()
rm(list = ls())
devtools::document()
devtools::document()
library(bcsnowstats)
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
# See package dependencies - how functions within the package relate to each other
#library(DependenciesGraphs)
#deps <- funDependencies("package:bcsnowstats", "generate_split_coded_comments")
#deps <- envirDependencies("package:bcsnowstats")
#plot(deps)
tims_start <- Sys.time()
test <- get_snow_stats(station_id = bcsnowdata::snow_auto_location()$LOCATION_ID[45:50],
survey_period = "All",
get_year = "2022",
normal_min = 1991,
normal_max = 2020,
force = TRUE)
time <- tims_start - Sys.time()
