} else {
print("No data for site")
station_diff_unlist <- NA
station_data_unlist <- NA
}
return(list(station_diff_unlist, station_data_unlist))
}
# get the statistics for the monthly change in SWE. Function returns both data as well as stats (stats = 1, data = 2)
monthly_df <- getmonthly_deltaSWE(id)
monthly_deltaSWE_data <- monthly_df[[2]]
#Get the monthly change in SWE - YTD
YTD_monthly_deltaSWE <- getmonthly_deltaSWE_YTD(id)
if (all(is.na(monthly_deltaSWE_data))) {
print("No data for station")
} else if (all(is.na(YTD_monthly_deltaSWE))) {
print("No data for station")
} else {
# Format so that the months start in Oct
monthly_deltaSWE_data$Month_order <- factor(monthly_deltaSWE_data$Month, levels = unique(monthly_deltaSWE_data$Month))
YTD_monthly_deltaSWE$Month_order <- factor(YTD_monthly_deltaSWE$Month, levels = unique(YTD_monthly_deltaSWE$Month))
years_data <- min(monthly_df[[1]]$n_years)
# plot the boxplot
p <- plotly::plot_ly(data = monthly_deltaSWE_data,
x = ~Month_order,
y = ~first_diff,
type = "box",
name = "Historic Monthly Accumulation") %>%
# Add the monthly accumulation for the YTD
plotly::add_trace(data = YTD_monthly_deltaSWE,
x = ~Month_order,
y = ~first_diff,
type = "scatter",
mode = "markers",
name = "YTD Monthly Accumulation") %>%
plotly::layout(title = paste0('Change in Monthly SWE (mm) for ', id),
xaxis = list(
title = 'Month'),
yaxis = list(title = 'Change in Monthly SWE (mm/month)')) %>%
# Add the number of years within the
plotly::layout(annotations = list(
list(x = 0 , y = 1, text = paste0("Years of data: ", years_data), showarrow = F, xref='paper', yref='paper')))
if (save %in% c("True", "true", "T", "TRUE", TRUE)) {
htmlwidgets::saveWidget(plotly::as_widget(p), paste0(path_save, "MonthlyDeltaSWE_", id, ".html"),
selfcontained = F, # for making the finised product smaller and faster to save
libdir = NULL, # for making the finised product smaller and faster to save
title = paste0("Monthly Delta SWE ", id))
}
return(p)
}
plot_monthly_deltaswe <- function(id, path_save, save = FALSE) {
# get the statistics for the monthly change in SWE. Function returns both data as well as stats (stats = 1, data = 2)
monthly_df <- getmonthly_deltaSWE(id)
monthly_deltaSWE_data <- monthly_df[[2]]
#Get the monthly change in SWE - YTD
YTD_monthly_deltaSWE <- getmonthly_deltaSWE_YTD(id)
if (all(is.na(monthly_deltaSWE_data))) {
print("No data for station")
} else if (all(is.na(YTD_monthly_deltaSWE))) {
print("No data for station")
} else {
# Format so that the months start in Oct
monthly_deltaSWE_data$Month_order <- factor(monthly_deltaSWE_data$Month, levels = unique(monthly_deltaSWE_data$Month))
YTD_monthly_deltaSWE$Month_order <- factor(YTD_monthly_deltaSWE$Month, levels = unique(YTD_monthly_deltaSWE$Month))
years_data <- min(monthly_df[[1]]$n_years)
# plot the boxplot
p <- plotly::plot_ly(data = monthly_deltaSWE_data,
x = ~Month_order,
y = ~first_diff,
type = "box",
name = "Historic Monthly Accumulation") %>%
# Add the monthly accumulation for the YTD
plotly::add_trace(data = YTD_monthly_deltaSWE,
x = ~Month_order,
y = ~first_diff,
type = "scatter",
mode = "markers",
name = "YTD Monthly Accumulation") %>%
plotly::layout(title = paste0('Change in Monthly SWE (mm) for ', id),
xaxis = list(
title = 'Month'),
yaxis = list(title = 'Change in Monthly SWE (mm/month)')) %>%
# Add the number of years within the
plotly::layout(annotations = list(
list(x = 0 , y = 1, text = paste0("Years of data: ", years_data), showarrow = F, xref='paper', yref='paper')))
if (save %in% c("True", "true", "T", "TRUE", TRUE)) {
htmlwidgets::saveWidget(plotly::as_widget(p), paste0(path_save, "MonthlyDeltaSWE_", id, ".html"),
selfcontained = F, # for making the finised product smaller and faster to save
libdir = NULL, # for making the finised product smaller and faster to save
title = paste0("Monthly Delta SWE ", id))
}
return(p)
}
}
test <- plot_monthly_deltaswe(id = ASWE_sites_active[1],
save = TRUE,
path_save = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/MonthlyDeltaSWE_boxplots/"))
lapply(ASWE_sites_active[1:5],
plot_monthly_deltaswe,
save = TRUE,
path_save = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/MonthlyDeltaSWE_boxplots/"))
lapply(ASWE_sites_active[23],
plot_monthly_deltaswe,
save = TRUE,
path_save = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/MonthlyDeltaSWE_boxplots/"))
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
library(dplyr)
# Test for interactive plots
# V drive for saving
drive = "\\\\DRAIN.dmz\\Shared"
drive_G = "\\\\Backhoe\\s63101\\Watershare\\rfc"
drive_Q = "\\\\question.bcgov\\envwwwt\\rfc"
drive_R = "\\\\answer.bcgov\\envwww\\rfc"
ASWE_sites <- bcsnowdata::snow_auto_location() %>%
dplyr::filter(STATUS == "Active")
ASWE_sites_active <- ASWE_sites$LOCATION_ID
tims_start2 <- Sys.time()
plot_test <- plot_interactive_aswe(id = ASWE_sites_active[12],
save = TRUE,
path = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/Interactive_plots/"))
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
library(dplyr)
# Test for interactive plots
# V drive for saving
drive = "\\\\DRAIN.dmz\\Shared"
drive_G = "\\\\Backhoe\\s63101\\Watershare\\rfc"
drive_Q = "\\\\question.bcgov\\envwwwt\\rfc"
drive_R = "\\\\answer.bcgov\\envwww\\rfc"
ASWE_sites <- bcsnowdata::snow_auto_location() %>%
dplyr::filter(STATUS == "Active")
ASWE_sites_active <- ASWE_sites$LOCATION_ID
# Try over all basins
lapply(snow_basins(),
plot_interactive_basin,
exceptions = NA,
path_basin = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/Basinaveraged_plots/"),
save = TRUE)
lapply(snow_basins(),
plot_interactive_basin,
exceptions = NA,
path_basin = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/Basinaveraged_plots/"),
save = TRUE)
lapply(snow_basins(),
plot_interactive_basin,
exceptions = NA,
path_basin = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/Basinaveraged_plots/"),
save = TRUE)
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
library(dplyr)
ASWE_sites <- bcsnowdata::snow_auto_location() %>%
dplyr::filter(STATUS == "Active")
ASWE_sites_active <- ASWE_sites$LOCATION_ID
normals_test <- SWE_normals(data = ASWE_sites_active[6],
normal_min = 1991,
normal_max = 2020,
force = FALSE)
data = ASWE_sites_active[6]
normal_min = 1991
normal_max = 2020
force = FALSE
aswe <- bcsnowdata::snow_auto_location()$LOCATION_ID
manual <- bcsnowdata::snow_manual_location()$LOCATION_ID
all(data %in% aswe)
# if the user input data as a station name (i.e., the function is being used as a stand alone function), get the data for the station
if (all(data %in% aswe)) {
data_norm <- bcsnowdata::get_aswe_databc(
station_id = data,
get_year = "All",
parameter = "swe",
timestep = "daily") %>%
dplyr::rename("values_stats" = value)
id <- data
} else if (all(data %in% manual)) {
data_norm <- bcsnowdata::get_manual_swe(
station_id = data,
get_year = "All",
survey_period = "All")
id <- data
} else {
data_norm <- data
if ("value" %in% colnames(data_norm)) {
data_norm <- data_norm %>%
dplyr::rename("values_stats" = value)
}
id <- unique(data_norm$id)
}
id
dim(data_norm)[1] == 0
any(id %in% aswe)
SWE_normals <- function(data, normal_max, normal_min, force = FALSE, ...) {
aswe <- bcsnowdata::snow_auto_location()$LOCATION_ID
manual <- bcsnowdata::snow_manual_location()$LOCATION_ID
# if the user input data as a station name (i.e., the function is being used as a stand alone function), get the data for the station
if (all(data %in% aswe)) {
data_norm <- bcsnowdata::get_aswe_databc(
station_id = data,
get_year = "All",
parameter = "swe",
timestep = "daily") %>%
dplyr::rename("values_stats" = value)
id <- data
} else if (all(data %in% manual)) {
data_norm <- bcsnowdata::get_manual_swe(
station_id = data,
get_year = "All",
survey_period = "All")
id <- data
} else {
data_norm <- data
if ("value" %in% colnames(data_norm)) {
data_norm <- data_norm %>%
dplyr::rename("values_stats" = value)
}
id <- unique(data_norm$id)
}
if (dim(data_norm)[1] == 0) {
df_normals_out <- data.frame(station_id = character())
} else if (any(id %in% aswe)) { # Check to see whether the station is a manual or automated station
#data_id <- "value"
# filter data for ASWE sites
data_swe <- data_norm %>%
dplyr::filter(id %in% aswe)
# Use the aswe_normal() function to fill in data (if appropriate) and calculate normals (if there is sufficient data)
df_normals_aswe <- aswe_normal(data = data_swe, normal_max, normal_min, data_id = "values_stats", force = force)
# If the site is manual site
} else if (any(id %in% manual)) {
data_id <- "swe_mm"
# filter data for manual sites
data_man <- data_norm %>%
dplyr::filter(id %in% manual)
df_normals_man <- manual_normal_prep(data = data_man, normal_max = normal_max, normal_min = normal_min, data_id = data_id, force)
} else if (id %in% snow_basins()) {
# if you are trying to simply get the normal for the entire basin, take the average across the data
df_normals_basin <- basin_normal(data = data_norm, normal_max = normal_max, normal_min = normal_min)
}
# If there is both aswe and manual, knit together. Otherwise, return the appropriate data
if (exists("df_normals_aswe") && exists("df_normals_man") ) {
df_normals_out <- list(df_normals_aswe, df_normals_man)
}
if (exists("df_normals_aswe") && !(exists("df_normals_man"))) {
df_normals_out <- df_normals_aswe
}
if (!(exists("df_normals_aswe")) && exists("df_normals_man")) {
df_normals_out <- df_normals_man
}
if (!(exists("df_normals_aswe")) && !(exists("df_normals_man")) && exists("df_normals_basin")) {
df_normals_out <- df_normals_basin
}
# End of function
return(df_normals_out)
}
normals_test <- SWE_normals(data = ASWE_sites_active[6],
normal_min = 1991,
normal_max = 2020,
force = FALSE)
normals_test <- SWE_normals(data = ASWE_sites_active[6],
normal_min = 1991,
normal_max = 2020,
force = FALSE)
data = ASWE_sites_active[6]
normal_min = 1991
normal_max = 2020
force = FALSE
aswe <- bcsnowdata::snow_auto_location()$LOCATION_ID
manual <- bcsnowdata::snow_manual_location()$LOCATION_ID
# if the user input data as a station name (i.e., the function is being used as a stand alone function), get the data for the station
if (all(data %in% aswe)) {
data_norm <- bcsnowdata::get_aswe_databc(
station_id = data,
get_year = "All",
parameter = "swe",
timestep = "daily") %>%
dplyr::rename("values_stats" = value)
id <- data
} else if (all(data %in% manual)) {
data_norm <- bcsnowdata::get_manual_swe(
station_id = data,
get_year = "All",
survey_period = "All")
id <- data
} else {
data_norm <- data
if ("value" %in% colnames(data_norm)) {
data_norm <- data_norm %>%
dplyr::rename("values_stats" = value)
}
id <- unique(data_norm$id)
}
if (dim(data_norm)[1] == 0) {
df_normals_out <- data.frame(station_id = character())
} else if (any(id %in% aswe)) { # Check to see whether the station is a manual or automated station
#data_id <- "value"
# filter data for ASWE sites
data_swe <- data_norm %>%
dplyr::filter(id %in% aswe)
# Use the aswe_normal() function to fill in data (if appropriate) and calculate normals (if there is sufficient data)
df_normals_aswe <- aswe_normal(data = data_swe, normal_max, normal_min, data_id = "values_stats", force = force)
# If the site is manual site
} else if (any(id %in% manual)) {
data_id <- "swe_mm"
# filter data for manual sites
data_man <- data_norm %>%
dplyr::filter(id %in% manual)
df_normals_man <- manual_normal_prep(data = data_man, normal_max = normal_max, normal_min = normal_min, data_id = data_id, force)
} else if (id %in% snow_basins()) {
# if you are trying to simply get the normal for the entire basin, take the average across the data
df_normals_basin <- basin_normal(data = data_norm, normal_max = normal_max, normal_min = normal_min)
}
dim(data_norm)[1] == 0
any(id %in% aswe)
# filter data for ASWE sites
data_swe <- data_norm %>%
dplyr::filter(id %in% aswe)
# Use the aswe_normal() function to fill in data (if appropriate) and calculate normals (if there is sufficient data)
df_normals_aswe <- aswe_normal(data = data_swe, normal_max, normal_min, data_id = "values_stats", force = force)
data = data_swe
data_id = "values_stats"
# Check to ensure that the ASWE archived data has been cached on the user's computer and is up to date
fname <- paste0(unique(data$parameter), "_norm_archive.rds")
dir <- data_dir()
fpath <- file.path(dir, fname)
data_dir <- function() {
if (R.Version()$major >= 4) {
getOption("bcsnowstats.data_dir", default = tools::R_user_dir("bcsnowstats", "cache"))
} else {
getOption("bcsnowstats.data_dir", default = rappdirs::user_cache_dir("bcsnowstats"))
}
}
show_cached_files <- function() {
file.path(list.files(data_dir(), full.names = TRUE))
}
check_write_to_data_dir <- function(dir, ask) {
if (ask) {
ans <- gtools::ask(paste("bcsnowstats would like to store this layer in the directory:",
dir, "Is that okay?", sep = "\n"))
if (!(ans %in% c("Yes", "YES", "yes", "y"))) stop("Exiting...", call. = FALSE)
}
if (!dir.exists(dir)) {
message("Creating directory to hold bcsnowstats data at ", dir)
dir.create(dir, showWarnings = FALSE, recursive = TRUE)
} else {
message("Saving to bcsnowstats data directory at ", dir)
}
}
# Check to ensure that the ASWE archived data has been cached on the user's computer and is up to date
fname <- paste0(unique(data$parameter), "_norm_archive.rds")
dir <- data_dir()
fpath <- file.path(dir, fname)
# If the file doesn't exists or the user decides to force the download, calculate the normal data for the station and save it
if (any(!file.exists(fpath)) | force) {
# Check that the directory exists
check_write_to_data_dir(dir, ask)
# Calculate the normal data for all days of the year
df_normals_out  <- int_aswenorm(data, normal_max, normal_min, data_id)
# Save archive - all data before current year
saveRDS(df_normals_out, fpath)
} else {
# Get the previously cached data
df_normals_initial <- readRDS(fpath)
# Check to ensure that the data contains statistics with the right normal range. Filter for the range you are looking for
check <- df_normals_initial %>%
dplyr::filter(initial_normal_range == paste0(normal_min, " to ", normal_max)) %>%
dplyr::filter(id %in% unique(data$id))
# If the archive doesn't have the normal range you want, get the data for your normal range and save it
if (dim(check)[1] < 1 | !(all(unique(check$id) %in% unique(data$id)))) {
# get normals for the year range you want
df_normals_out  <- int_aswenorm(data, normal_max, normal_min, data_id)
if (!is.null(df_normals_out)) {
# Append to the data for the other normal range calculated and save
df_normals_save <- dplyr::full_join(df_normals_out, df_normals_initial) %>%
unique() %>%
dplyr::arrange(m_d)
saveRDS(df_normals_save, fpath)
}
} else {
df_normals_out <- check
}
}
aswe_normal <- function(df, normal_max, normal_min, data_id, ask = FALSE, force = FALSE) {
# Check cached data to see if the normals have already been calculated for the day of interest
# Check to ensure that the ASWE archived data has been cached on the user's computer and is up to date
fname <- paste0(unique(df$parameter), "_norm_archive.rds")
dir <- data_dir()
fpath <- file.path(dir, fname)
# If the file doesn't exists or the user decides to force the download, calculate the normal data for the station and save it
if (any(!file.exists(fpath)) | force) {
# Check that the directory exists
check_write_to_data_dir(dir, ask)
# Calculate the normal data for all days of the year
df_normals_out  <- int_aswenorm(df, normal_max, normal_min, data_id)
# Save archive - all data before current year
saveRDS(df_normals_out, fpath)
} else {
# Get the previously cached data
df_normals_initial <- readRDS(fpath)
# Check to ensure that the data contains statistics with the right normal range. Filter for the range you are looking for
check <- df_normals_initial %>%
dplyr::filter(initial_normal_range == paste0(normal_min, " to ", normal_max)) %>%
dplyr::filter(id %in% unique(df$id))
# If the archive doesn't have the normal range you want, get the data for your normal range and save it
if (dim(check)[1] < 1 | !(all(unique(check$id) %in% unique(df$id)))) {
# get normals for the year range you want
df_normals_out  <- int_aswenorm(df, normal_max, normal_min, data_id)
if (!is.null(df_normals_out)) {
# Append to the data for the other normal range calculated and save
df_normals_save <- dplyr::full_join(df_normals_out, df_normals_initial) %>%
unique() %>%
dplyr::arrange(m_d)
saveRDS(df_normals_save, fpath)
}
} else {
df_normals_out <- check
}
}
return(df_normals_out)
}
#' Function for actually calculating normals from ASWE stations
#' January 2021, Ashlee Jollymore
#' @param data station that you are calculating statistics for
#' @param normal_min date for the min normal year
#' @param normal_max date of the max normal year
#' @param data_id column name of data you are calculating normals for
#' @export
#' @keywords internal
#' @examples \dontrun{}
#
int_aswenorm <- function(data, normal_max, normal_min, data_id) {
# Put data into right format using the data_massage function
data_m <- data_massage(data)
if ("swe_mean" %in% colnames(data_m)) {
data_id <- "swe_mean" # reassign the data_ID value
}
# Filter the data by the normal span that you specify
df_normal_time <- data_m %>%
dplyr::filter(wr <= normal_max, wr >= normal_min) %>% # Filter by the normal dates that you specify
dplyr::group_by(id, m_d) %>%
dplyr::rename(values_stats = all_of(data_id))
# ++++++++++++++++++++++ thresholds
# Check to see whether there is sufficient data to calculate a normal.
# The WMO recommends only calculating a normal for stations that have 80% of the data available
# Firstly, just show the amount of data available for the normal period
# Number of years with 80% or great of the data available.
# Only count the data between Oct-June - doesn't matter if the snow data is missing in summer - 273 days in snow accumulation/melt season
# Only for ASWE
df_normal_80 <- df_normal_time %>%
dplyr::filter(!is.na(values_stats)) %>% # # filter out missing data
dplyr::ungroup() %>%
dplyr::group_by(id, wr) %>%
dplyr::filter(lubridate::month(as.Date(m_d, format = "%m-%d")) <= 6 || lubridate::month(as.Date(m_d, format = "%m-%d")) >= 10) %>% # get only the snow accumulation and melt season
dplyr::mutate(percent_available = length(values_stats) / length(seq(as.Date("2020-10-01"), as.Date("2021-06-30"), by = "day")) * 100) %>%
dplyr::select(id, wr, percent_available) %>%
unique() %>%
dplyr::filter(percent_available >= 80) # filter by the 80% WMO threshold
# Get the number of years within the normal range with >= 80% data coverage within a specific year
ny_80 <- df_normal_80 %>%
dplyr::group_by(id) %>%
dplyr::summarize(numberofyears_80_raw = n())
# Add the number of years with 80% of data to the dataframe
df_nt <- df_normal_time %>%
dplyr::full_join(ny_80) %>%
dplyr::mutate(numberofyears_80_raw = ifelse(is.na(numberofyears_80_raw), 0, numberofyears_80_raw))
normals <- lapply(unique(data$id),
calc_norm,
df_nt,
df_normal_80, normal_max = normal_max, normal_min = normal_min)
df_normals_out <- do.call(rbind, normals)
return(df_normals_out)
}
# Use the aswe_normal() function to fill in data (if appropriate) and calculate normals (if there is sufficient data)
df_normals_aswe <- aswe_normal(data = data_swe, normal_max, normal_min, data_id = "values_stats", force = force)
# Use the aswe_normal() function to fill in data (if appropriate) and calculate normals (if there is sufficient data)
df_normals_aswe <- aswe_normal(df = data_swe, normal_max, normal_min, data_id = "values_stats", force = force)
View(df_normals_aswe)
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
library(dplyr)
library(bcsnowstats)
# Try the normals
normals_test <- SWE_normals(data = ASWE_sites_active[6],
normal_min = 1991,
normal_max = 2020,
force = FALSE)
ASWE_sites <- bcsnowdata::snow_auto_location() %>%
dplyr::filter(STATUS == "Active")
ASWE_sites_active <- ASWE_sites$LOCATION_ID
id_test = bcsnowdata::snow_auto_location()$LOCATION_ID[10]
normals_test <- SWE_normals(data = ASWE_sites_active[6],
normal_min = 1991,
normal_max = 2020,
force = FALSE)
# Test for interactive plots
# V drive for saving
drive = "\\\\DRAIN.dmz\\Shared"
drive_G = "\\\\Backhoe\\s63101\\Watershare\\rfc"
drive_Q = "\\\\question.bcgov\\envwwwt\\rfc"
drive_R = "\\\\answer.bcgov\\envwww\\rfc"
ASWE_sites <- bcsnowdata::snow_auto_location() %>%
dplyr::filter(STATUS == "Active")
ASWE_sites_active <- ASWE_sites$LOCATION_ID
tims_start2 <- Sys.time()
plot_test <- plot_interactive_aswe(id = ASWE_sites_active,
save = TRUE,
path = paste0(drive_Q, "/Real-time_Data/ASP_daily_interactive/ASWE/Interactive_plots/"))
time_save
time_save <- tims_start2 - Sys.time()
time_save
