dplyr::mutate(percent_available = length(values_stats) / length(seq(as.Date("2020-10-01"), as.Date("2021-06-30"), by = "day")) * 100)
View(df_normal_80)
df_normal_80 <- df_normal_time %>%
dplyr::filter(!is.na(values_stats)) %>% # # filter out missing data
dplyr::ungroup() %>%
dplyr::group_by(id, wr) %>%
dplyr::filter(lubridate::month(as.Date(m_d, format = "%m-%d")) <= 6 || lubridate::month(as.Date(m_d, format = "%m-%d")) >= 10) %>% # get only the snow accumulation and melt season
dplyr::mutate(percent_available = length(values_stats) / length(seq(as.Date("2020-10-01"), as.Date("2021-06-30"), by = "day")) * 100) %>%
dplyr::select(id, wr, percent_available)
df_normal_80 <- df_normal_time %>%
dplyr::filter(!is.na(values_stats)) %>% # # filter out missing data
dplyr::ungroup() %>%
dplyr::group_by(id, wr) %>%
dplyr::filter(lubridate::month(as.Date(m_d, format = "%m-%d")) <= 6 || lubridate::month(as.Date(m_d, format = "%m-%d")) >= 10) %>% # get only the snow accumulation and melt season
dplyr::mutate(percent_available = length(values_stats) / length(seq(as.Date("2020-10-01"), as.Date("2021-06-30"), by = "day")) * 100) %>%
dplyr::select(id, wr, percent_available) %>%
unique()
df_normal_80 <- df_normal_time %>%
dplyr::filter(!is.na(values_stats)) %>% # # filter out missing data
dplyr::ungroup() %>%
dplyr::group_by(id, wr) %>%
dplyr::filter(lubridate::month(as.Date(m_d, format = "%m-%d")) <= 6 || lubridate::month(as.Date(m_d, format = "%m-%d")) >= 10) %>% # get only the snow accumulation and melt season
dplyr::mutate(percent_available = length(values_stats) / length(seq(as.Date("2020-10-01"), as.Date("2021-06-30"), by = "day")) * 100) %>%
dplyr::select(id, wr, percent_available) %>%
unique() %>%
dplyr::filter(percent_available >= 80)
dim(df_normal_80)[1]
numberofyears_80 <- df_normal_80 %>%
dplyr::group_by(id) %>%
dplyr::summarize(numberofyears_80 = length(wr))
View(df_normal_80)
numberofyears_80 <- df_normal_80 %>%
dplyr::group_by(id) %>%
dplyr::summarize(numberofyears_80 = n())
View(numberofyears_80)
numberofyears_80 <- df_normal_80 %>%
dplyr::group_by(id) %>%
dplyr::summarize(numberofyears_80_raw = n())
df_normal_time <- df_normal_time %>%
dplyr::full_join(numberofyears_80)
View(df_normal_time)
unique(df_normal_time$id)
calc_norm <- function(numberofyears_80, df_normal_time) {
# =============================
# Fill in data depending on how many years of data there are available
# Is there less than 10 years of data?
if (numberofyears_80 < 10) {
data_0t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# ++++++++++++++++++++++++++++++++++++++++++++++
# Use function to check to see if there is manual site to extend data.
# For now, do not calculate a normal
# Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columms
all_swe <- data_0t10 %>%
dplyr::filter(wr %in% df_normal_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# Does the station have between 10-20 years of data? If so, extend the dataset using 1) manual dataset (if converted), and 2) adjacent stations
if (numberofyears_80 >= 10 && numberofyears_80 < 20) {
data_20t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# Fill in missing data with an estimated dataset from either manual dataset (if converted) and/or adjacent stations
all_swe <- snow_datafill(data_soi = data_20t10, data_id, normal_max, normal_min)
}
# Does the site have between 20-30 years of data? Don't add in any additional data and jsut calculcate normals from
if (numberofyears_80 >= 20 && numberofyears_80 <= 30) {
# DON'T Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columns
all_swe <- df_normal_time %>%
#dplyr::filter(wr %in% df_normal_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# End of data filling according to thresholds
# ==============================
# Calculate normals. Only calculate normal if there is sufficient data
if (length(all_swe$numberofyears_estimated_80) > 0 && unique(all_swe$numberofyears_estimated_80) >= 20 && unique(all_swe$numberofyears_estimated_80) <= 30) {
all_swe_1 <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::filter(!is.na(swe_fornormal))
# Calculate the normal statistics for each day of the year
df_normals <- do.call(data.frame,
list(dplyr::summarise(all_swe_1, normal_minimum = min(swe_fornormal, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_swe_mean = mean(swe_fornormal, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q5 = quantile(swe_fornormal, 0.05, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q10 = quantile(swe_fornormal, 0.1, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q25 = quantile(swe_fornormal, 0.25, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q50 = quantile(swe_fornormal, 0.5, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q75 = quantile(swe_fornormal, 0.75, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q90 = quantile(swe_fornormal, 0.90, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_maximum = max(swe_fornormal, na.rm = TRUE), .groups = "keep"))) %>%
dplyr::select(-m_d.1, -m_d.2, -m_d.3, -m_d.4, -m_d.5, -m_d.6, -m_d.7, -m_d.8) %>%
dplyr::select(-id.1, -id.2, -id.3, -id.4, -id.5, -id.6, -id.7, -id.8) %>%
#dplyr::mutate(Data_Range_normal = (paste0(round(normal_minimum, digits = 0), ' to ', round(normal_maximum, digits = 0)))) %>%
dplyr::mutate(data_range_normal = (paste0(min(lubridate::year(all_swe$date_utc), na.rm = TRUE), " to ", max(lubridate::year(all_swe$date_utc), na.rm = TRUE)))) %>%
dplyr::mutate(normal_datarange_estimated = unique(all_swe$numberofyears_estimated_80, na.rm = TRUE)[!is.na(unique(all_swe$numberofyears_estimated_80, na.rm = TRUE))]) %>%
dplyr::mutate(normal_datarange_raw = unique(all_swe$numberofyears_80_raw, na.rm = TRUE)[!is.na(unique(all_swe$numberofyears_80_raw, na.rm = TRUE))])
# get the day of the max and min!! Use only 'real', non estimated data
min_date <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.min(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_min_normal_utc = date_utc)
max_date <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.max(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_max_normal_utc = date_utc)
# append to data
dates <- dplyr::full_join(min_date, max_date, by = c("id", "m_d"))
df_normals_out <- dplyr::full_join(df_normals, dates, by = c("id", "m_d")) %>%
dplyr::mutate(initial_normal_range = paste0(normal_min, " to ", normal_max))
# Smooth all the statistics by the 5 -day average?
# If there is less than 10 years of data available even after trying adjacent sites, return
} else {
df_normals_out <- data.frame("id"  = unique(data$id),
"m_d" = NA,
"normal_minimum" = NA,
"normal_swe_mean" = NA,
"normal_Q5" = NA,
"normal_Q10" = NA,
"normal_Q25"  = NA,
"normal_Q50" = NA,
"normal_Q75" = NA,
"normal_Q90" = NA,
"normal_maximum" = NA,
"data_range_normal" = NA,
"initial_normal_range" = paste0(normal_min, " to ", normal_max),
"normal_datarange_estimated" = NA,
"normal_datarange_raw" = NA,
"date_min_normal_utc" = NA,
"date_max_normal_utc"  = NA)
}
df_normals_out
}
# Get the number of years within the normal range with >= 80% data coverage within a specific year
#numberofyears_80 <- dim(df_normal_80)[1]
ny_80 <- df_normal_80 %>%
dplyr::group_by(id) %>%
dplyr::summarize(numberofyears_80_raw = n())
# Add the number of years with 80% of data to the dataframe
df_nt <- df_normal_time %>%
dplyr::full_join(numberofyears_80) $numberofyears_80_raw <- dim(df_normal_80)[1]
# Put data into right format using the data_massage function
data_m <- data_massage(data)
if ("swe_mean" %in% colnames(data_m)) {
data_id <- "swe_mean" # reassign the data_ID value
}
# Filter the data by the normal span that you specify
df_normal_time <- data_m %>%
dplyr::filter(wr <= normal_max, wr >= normal_min) %>% # Filter by the normal dates that you specify
dplyr::group_by(id, m_d) %>%
dplyr::rename(values_stats = all_of(data_id))
# ++++++++++++++++++++++ thresholds
# Check to see whether there is sufficient data to calculate a normal.
# The WMO recommends only calculating a normal for stations that have 80% of the data available
# Firstly, just show the amount of data available for the normal period
# Number of years with 80% or great of the data available.
# Only count the data between Oct-June - doesn't matter if the snow data is missing in summer - 273 days in snow accumulation/melt season
# Only for ASWE
df_normal_80 <- df_normal_time %>%
dplyr::filter(!is.na(values_stats)) %>% # # filter out missing data
dplyr::ungroup() %>%
dplyr::group_by(id, wr) %>%
dplyr::filter(lubridate::month(as.Date(m_d, format = "%m-%d")) <= 6 || lubridate::month(as.Date(m_d, format = "%m-%d")) >= 10) %>% # get only the snow accumulation and melt season
dplyr::mutate(percent_available = length(values_stats) / length(seq(as.Date("2020-10-01"), as.Date("2021-06-30"), by = "day")) * 100) %>%
dplyr::select(id, wr, percent_available) %>%
unique() %>%
dplyr::filter(percent_available >= 80) # filter by the 80% WMO threshold
# Get the number of years within the normal range with >= 80% data coverage within a specific year
#numberofyears_80 <- dim(df_normal_80)[1]
ny_80 <- df_normal_80 %>%
dplyr::group_by(id) %>%
dplyr::summarize(numberofyears_80_raw = n())
# Add the number of years with 80% of data to the dataframe
df_nt <- df_normal_time %>%
dplyr::full_join(numberofyears_80)
unique(df_nt$id)
calc_norm <- function(station, ny_80, df_nt) {
numberofyears_80 <- ny_80 %>%
dplyr::filter(id %in% station)
df_normal_time <- df_nt %>%
dplyr::filter(id %in% station)
# =============================
# Fill in data depending on how many years of data there are available
# Is there less than 10 years of data?
if (numberofyears_80 < 10) {
data_0t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# ++++++++++++++++++++++++++++++++++++++++++++++
# Use function to check to see if there is manual site to extend data.
# For now, do not calculate a normal
# Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columms
all_swe <- data_0t10 %>%
dplyr::filter(wr %in% df_normal_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# Does the station have between 10-20 years of data? If so, extend the dataset using 1) manual dataset (if converted), and 2) adjacent stations
if (numberofyears_80 >= 10 && numberofyears_80 < 20) {
data_20t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# Fill in missing data with an estimated dataset from either manual dataset (if converted) and/or adjacent stations
all_swe <- snow_datafill(data_soi = data_20t10, data_id, normal_max, normal_min)
}
# Does the site have between 20-30 years of data? Don't add in any additional data and jsut calculcate normals from
if (numberofyears_80 >= 20 && numberofyears_80 <= 30) {
# DON'T Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columns
all_swe <- df_normal_time %>%
#dplyr::filter(wr %in% df_normal_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# End of data filling according to thresholds
# ==============================
# Calculate normals. Only calculate normal if there is sufficient data
if (length(all_swe$numberofyears_estimated_80) > 0 && unique(all_swe$numberofyears_estimated_80) >= 20 && unique(all_swe$numberofyears_estimated_80) <= 30) {
all_swe_1 <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::filter(!is.na(swe_fornormal))
# Calculate the normal statistics for each day of the year
df_normals <- do.call(data.frame,
list(dplyr::summarise(all_swe_1, normal_minimum = min(swe_fornormal, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_swe_mean = mean(swe_fornormal, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q5 = quantile(swe_fornormal, 0.05, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q10 = quantile(swe_fornormal, 0.1, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q25 = quantile(swe_fornormal, 0.25, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q50 = quantile(swe_fornormal, 0.5, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q75 = quantile(swe_fornormal, 0.75, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q90 = quantile(swe_fornormal, 0.90, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_maximum = max(swe_fornormal, na.rm = TRUE), .groups = "keep"))) %>%
dplyr::select(-m_d.1, -m_d.2, -m_d.3, -m_d.4, -m_d.5, -m_d.6, -m_d.7, -m_d.8) %>%
dplyr::select(-id.1, -id.2, -id.3, -id.4, -id.5, -id.6, -id.7, -id.8) %>%
#dplyr::mutate(Data_Range_normal = (paste0(round(normal_minimum, digits = 0), ' to ', round(normal_maximum, digits = 0)))) %>%
dplyr::mutate(data_range_normal = (paste0(min(lubridate::year(all_swe$date_utc), na.rm = TRUE), " to ", max(lubridate::year(all_swe$date_utc), na.rm = TRUE)))) %>%
dplyr::mutate(normal_datarange_estimated = unique(all_swe$numberofyears_estimated_80, na.rm = TRUE)[!is.na(unique(all_swe$numberofyears_estimated_80, na.rm = TRUE))]) %>%
dplyr::mutate(normal_datarange_raw = unique(all_swe$numberofyears_80_raw, na.rm = TRUE)[!is.na(unique(all_swe$numberofyears_80_raw, na.rm = TRUE))])
# get the day of the max and min!! Use only 'real', non estimated data
min_date <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.min(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_min_normal_utc = date_utc)
max_date <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.max(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_max_normal_utc = date_utc)
# append to data
dates <- dplyr::full_join(min_date, max_date, by = c("id", "m_d"))
df_normals_out <- dplyr::full_join(df_normals, dates, by = c("id", "m_d")) %>%
dplyr::mutate(initial_normal_range = paste0(normal_min, " to ", normal_max))
# Smooth all the statistics by the 5 -day average?
# If there is less than 10 years of data available even after trying adjacent sites, return
} else {
df_normals_out <- data.frame("id"  = unique(data$id),
"m_d" = NA,
"normal_minimum" = NA,
"normal_swe_mean" = NA,
"normal_Q5" = NA,
"normal_Q10" = NA,
"normal_Q25"  = NA,
"normal_Q50" = NA,
"normal_Q75" = NA,
"normal_Q90" = NA,
"normal_maximum" = NA,
"data_range_normal" = NA,
"initial_normal_range" = paste0(normal_min, " to ", normal_max),
"normal_datarange_estimated" = NA,
"normal_datarange_raw" = NA,
"date_min_normal_utc" = NA,
"date_max_normal_utc"  = NA)
}
df_normals_out
}
unique(df_nt$id)
# Add the number of years with 80% of data to the dataframe
df_nt <- df_normal_time %>%
dplyr::full_join(numberofyears_80)
normals <- lapply(unique(df_nt$id),
calc_norm,
ny_80,
df_nt)
station = unique(df_nt$id)[1]
station
ny_80
df_nt
station
ny_80
df_nt
numberofyears_80 <- ny_80 %>%
dplyr::filter(id %in% station)
View(numberofyears_80)
df_normal_time <- df_nt %>%
dplyr::filter(id %in% station)
numberofyears_80
numberofyears_80$numberofyears_80_raw < 10
df_normal_80
numberofyears_80 <- ny_80 %>%
dplyr::filter(id %in% station)
df_normal_time <- df_nt %>%
dplyr::filter(id %in% station)
numberofyears_80 <- ny_80 %>%
dplyr::filter(id %in% station)
df_normal_time <- df_nt %>%
dplyr::filter(id %in% station)
dfn_80 <- df_normal_80 %>%
dplyr::filter(id %in% station)
# =============================
# Fill in data depending on how many years of data there are available
# Is there less than 10 years of data?
if (numberofyears_80$numberofyears_80_raw < 10) {
data_0t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# ++++++++++++++++++++++++++++++++++++++++++++++
# Use function to check to see if there is manual site to extend data.
# For now, do not calculate a normal
# Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columms
all_swe <- data_0t10 %>%
dplyr::filter(wr %in% dfn_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
numberofyears_80$numberofyears_80_raw >= 10 && numberofyears_80$numberofyears_80_raw < 20
# Does the site have between 20-30 years of data? Don't add in any additional data and jsut calculcate normals from
if (numberofyears_80$numberofyears_80_raw >= 20 && numberofyears_80$numberofyears_80_raw <= 30) {
# DON'T Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columns
all_swe <- df_normal_time %>%
#dplyr::filter(wr %in% df_normal_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80$numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
View(all_swe)
all_swe$numberofyears_estimated_80
# ==============================
# Calculate normals. Only calculate normal if there is sufficient data
if (length(all_swe$numberofyears_estimated_80) > 0 && unique(all_swe$numberofyears_estimated_80) >= 20 && unique(all_swe$numberofyears_estimated_80) <= 30) {
all_swe_1 <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::filter(!is.na(swe_fornormal))
# Calculate the normal statistics for each day of the year
df_normals <- do.call(data.frame,
list(dplyr::summarise(all_swe_1, normal_minimum = min(swe_fornormal, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_swe_mean = mean(swe_fornormal, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q5 = quantile(swe_fornormal, 0.05, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q10 = quantile(swe_fornormal, 0.1, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q25 = quantile(swe_fornormal, 0.25, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q50 = quantile(swe_fornormal, 0.5, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q75 = quantile(swe_fornormal, 0.75, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q90 = quantile(swe_fornormal, 0.90, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_maximum = max(swe_fornormal, na.rm = TRUE), .groups = "keep"))) %>%
dplyr::select(-m_d.1, -m_d.2, -m_d.3, -m_d.4, -m_d.5, -m_d.6, -m_d.7, -m_d.8) %>%
dplyr::select(-id.1, -id.2, -id.3, -id.4, -id.5, -id.6, -id.7, -id.8) %>%
#dplyr::mutate(Data_Range_normal = (paste0(round(normal_minimum, digits = 0), ' to ', round(normal_maximum, digits = 0)))) %>%
dplyr::mutate(data_range_normal = (paste0(min(lubridate::year(all_swe$date_utc), na.rm = TRUE), " to ", max(lubridate::year(all_swe$date_utc), na.rm = TRUE)))) %>%
dplyr::mutate(normal_datarange_estimated = unique(all_swe$numberofyears_estimated_80, na.rm = TRUE)[!is.na(unique(all_swe$numberofyears_estimated_80, na.rm = TRUE))]) %>%
dplyr::mutate(normal_datarange_raw = unique(all_swe$numberofyears_80_raw, na.rm = TRUE)[!is.na(unique(all_swe$numberofyears_80_raw, na.rm = TRUE))])
# get the day of the max and min!! Use only 'real', non estimated data
min_date <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.min(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_min_normal_utc = date_utc)
max_date <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.max(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_max_normal_utc = date_utc)
# append to data
dates <- dplyr::full_join(min_date, max_date, by = c("id", "m_d"))
df_normals_out <- dplyr::full_join(df_normals, dates, by = c("id", "m_d")) %>%
dplyr::mutate(initial_normal_range = paste0(normal_min, " to ", normal_max))
# Smooth all the statistics by the 5 -day average?
# If there is less than 10 years of data available even after trying adjacent sites, return
} else {
df_normals_out <- data.frame("id"  = unique(data$id),
"m_d" = NA,
"normal_minimum" = NA,
"normal_swe_mean" = NA,
"normal_Q5" = NA,
"normal_Q10" = NA,
"normal_Q25"  = NA,
"normal_Q50" = NA,
"normal_Q75" = NA,
"normal_Q90" = NA,
"normal_maximum" = NA,
"data_range_normal" = NA,
"initial_normal_range" = paste0(normal_min, " to ", normal_max),
"normal_datarange_estimated" = NA,
"normal_datarange_raw" = NA,
"date_min_normal_utc" = NA,
"date_max_normal_utc"  = NA)
}
calc_norm <- function(station, ny_80, df_nt, df_normal_80) {
numberofyears_80 <- ny_80 %>%
dplyr::filter(id %in% station)
df_normal_time <- df_nt %>%
dplyr::filter(id %in% station)
dfn_80 <- df_normal_80 %>%
dplyr::filter(id %in% station)
# =============================
# Fill in data depending on how many years of data there are available
# Is there less than 10 years of data?
if (numberofyears_80$numberofyears_80_raw < 10) {
data_0t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# ++++++++++++++++++++++++++++++++++++++++++++++
# Use function to check to see if there is manual site to extend data.
# For now, do not calculate a normal
# Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columms
all_swe <- data_0t10 %>%
dplyr::filter(wr %in% dfn_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# Does the station have between 10-20 years of data? If so, extend the dataset using 1) manual dataset (if converted), and 2) adjacent stations
if (numberofyears_80$numberofyears_80_raw >= 10 && numberofyears_80$numberofyears_80_raw < 20) {
data_20t10 <- df_normal_time # Make a new variable to preserve the initial data - years with at least 80% of the data in the snow accumulation period.
# Fill in missing data with an estimated dataset from either manual dataset (if converted) and/or adjacent stations
all_swe <- snow_datafill(data_soi = data_20t10, data_id, normal_max, normal_min)
}
# Does the site have between 20-30 years of data? Don't add in any additional data and jsut calculcate normals from
if (numberofyears_80$numberofyears_80_raw >= 20 && numberofyears_80$numberofyears_80_raw <= 30) {
# DON'T Filter out the years that have less that 80% of the data within the snow accumulation season; Add in correct columns
all_swe <- df_normal_time %>%
#dplyr::filter(wr %in% df_normal_80$wr) %>%
dplyr::mutate(numberofyears_estimated_80 = numberofyears_80$numberofyears_80_raw) %>%
dplyr::mutate(swe_fornormal = values_stats)
}
# End of data filling according to thresholds
# ==============================
# Calculate normals. Only calculate normal if there is sufficient data
if (length(all_swe$numberofyears_estimated_80) > 0 && unique(all_swe$numberofyears_estimated_80) >= 20 && unique(all_swe$numberofyears_estimated_80) <= 30) {
all_swe_1 <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::filter(!is.na(swe_fornormal))
# Calculate the normal statistics for each day of the year
df_normals <- do.call(data.frame,
list(dplyr::summarise(all_swe_1, normal_minimum = min(swe_fornormal, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_swe_mean = mean(swe_fornormal, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q5 = quantile(swe_fornormal, 0.05, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q10 = quantile(swe_fornormal, 0.1, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q25 = quantile(swe_fornormal, 0.25, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q50 = quantile(swe_fornormal, 0.5, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q75 = quantile(swe_fornormal, 0.75, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_Q90 = quantile(swe_fornormal, 0.90, na.rm = TRUE), .groups = "keep"),
dplyr::summarise(all_swe_1, normal_maximum = max(swe_fornormal, na.rm = TRUE), .groups = "keep"))) %>%
dplyr::select(-m_d.1, -m_d.2, -m_d.3, -m_d.4, -m_d.5, -m_d.6, -m_d.7, -m_d.8) %>%
dplyr::select(-id.1, -id.2, -id.3, -id.4, -id.5, -id.6, -id.7, -id.8) %>%
#dplyr::mutate(Data_Range_normal = (paste0(round(normal_minimum, digits = 0), ' to ', round(normal_maximum, digits = 0)))) %>%
dplyr::mutate(data_range_normal = (paste0(min(lubridate::year(all_swe$date_utc), na.rm = TRUE), " to ", max(lubridate::year(all_swe$date_utc), na.rm = TRUE)))) %>%
dplyr::mutate(normal_datarange_estimated = unique(all_swe$numberofyears_estimated_80, na.rm = TRUE)[!is.na(unique(all_swe$numberofyears_estimated_80, na.rm = TRUE))]) %>%
dplyr::mutate(normal_datarange_raw = unique(all_swe$numberofyears_80_raw, na.rm = TRUE)[!is.na(unique(all_swe$numberofyears_80_raw, na.rm = TRUE))])
# get the day of the max and min!! Use only 'real', non estimated data
min_date <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.min(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_min_normal_utc = date_utc)
max_date <- all_swe %>%
dplyr::group_by(id, m_d) %>%
dplyr::slice(which.max(values_stats)) %>%
dplyr::select(date_utc, id, m_d) %>%
dplyr::rename(date_max_normal_utc = date_utc)
# append to data
dates <- dplyr::full_join(min_date, max_date, by = c("id", "m_d"))
df_normals_out <- dplyr::full_join(df_normals, dates, by = c("id", "m_d")) %>%
dplyr::mutate(initial_normal_range = paste0(normal_min, " to ", normal_max))
# Smooth all the statistics by the 5 -day average?
# If there is less than 10 years of data available even after trying adjacent sites, return
} else {
df_normals_out <- data.frame("id"  = unique(data$id),
"m_d" = NA,
"normal_minimum" = NA,
"normal_swe_mean" = NA,
"normal_Q5" = NA,
"normal_Q10" = NA,
"normal_Q25"  = NA,
"normal_Q50" = NA,
"normal_Q75" = NA,
"normal_Q90" = NA,
"normal_maximum" = NA,
"data_range_normal" = NA,
"initial_normal_range" = paste0(normal_min, " to ", normal_max),
"normal_datarange_estimated" = NA,
"normal_datarange_raw" = NA,
"date_min_normal_utc" = NA,
"date_max_normal_utc"  = NA)
}
df_normals_out
}
normals <- lapply(unique(df_nt$id),
calc_norm,
ny_80,
df_nt,
df_normal_80)
df_normals_out <- do.call(rbind, normals)
View(df_normals_out)
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
library(dplyr)
ASWE_sites <- bcsnowdata::snow_auto_location() %>%
dplyr::filter(STATUS == "Active")
ASWE_sites_active <- ASWE_sites$LOCATION_ID
id_test = bcsnowdata::snow_auto_location()$LOCATION_ID[10]
tims_start <- Sys.time()
test <- get_snow_stats(station_id = ASWE_sites_active[3],
survey_period = "All",
get_year = "2022",
normal_min = 1991,
normal_max = 2020,
force = TRUE)
time_1 <- tims_start - Sys.time()
rm(list = ls())
library(bcsnowdata)
library(bcsnowstats)
library(dplyr)
